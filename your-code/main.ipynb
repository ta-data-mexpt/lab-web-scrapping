{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Ssssssssoup.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['Tim Paine (timkpaine)',\n 'Kyle Mathews (KyleAMathews)',\n 'XhmikosR (ForbesLindesay)',\n 'Forbes Lindesay (oznu)',\n 'oznu (jethrokuan)',\n 'Jethro Kuan (zhanghang1989)',\n 'Hang Zhang (chrisbanes)',\n 'Chris Banes (whyrusleeping)',\n 'Whyrusleeping (thestr4ng3r)',\n 'Florian Märkl (muukii)',\n 'Hiroshi Kimura (bwplotka)',\n 'Bartlomiej Plotka (wcandillon)',\n 'William Candillon (saschagrunert)',\n 'Sascha Grunert (borkdude)',\n 'Michiel Borkent (tasomaniac)',\n 'Said Tahsin Dane (schneems)',\n 'Richard Schneeman (Airblader)',\n 'Ingo Bürk (meyskens)',\n 'Maartje Eyskens (angristan)',\n 'Stanislas (msfjarvis)',\n 'Harsh Shandilya (developit)',\n 'Jason Miller (yuche)',\n 'Yu Che (marten-seemann)',\n 'Marten Seemann (cristianoc)']"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "#Developer's names.\n",
    "devs = soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "devs_list = [dev.text.replace('\\n\\n','\\n').strip().split(\"\\n\") for dev in devs]\n",
    "devs_flat = [item for sublist in devs_list for item in sublist]\n",
    "\n",
    "#Developer's Usernames.\n",
    "devs_usernames = soup.find_all('p',{'class':'f4 text-normal mb-1'})\n",
    "devs_usernames_list = [dev.text.replace('\\n\\n','\\n').strip().split(\"\\n\") for dev in devs_usernames]\n",
    "devs_usernames_flat = ['('+item+')' for sublist in devs_usernames_list for item in sublist]\n",
    "\n",
    "#Unite everything into a list\n",
    "trending_devs_zip =  list(zip(devs_flat, devs_usernames_flat))\n",
    "trending_devs = [' '.join(map(str,i)) for i in trending_devs_zip]\n",
    "trending_devs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Soupppp II.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['podgorskiy / ALAE',\n 'timgrossmann / InstaPy',\n '3b1b / manim',\n 'TheAlgorithms / Python',\n 'donnemartin / system-design-primer',\n 'TachibanaYoshino / AnimeGAN',\n 'alexgand / springer_free_books',\n 'soimort / you-get',\n 'scikit-learn / scikit-learn',\n 'freqtrade / freqtrade',\n 'NVlabs / stylegan2',\n 'programthink / zhao',\n 'volatilityfoundation / volatility',\n 'Manisso / fsociety',\n 'PyTorchLightning / pytorch-lightning',\n 'OctoPrint / OctoPrint',\n 'iperov / DeepFaceLab',\n 'matplotlib / mplfinance',\n 'NVIDIA / apex',\n 'Alic-yuan / nlp-beginner-finish',\n 'commaai / comma10k',\n 'ycm-core / YouCompleteMe',\n 'apprenticeharper / DeDRM_tools',\n 'google-research / bert',\n 'neha-chawla / RandomPython']"
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "#Trending Python repositories in GitHub.\n",
    "reps = soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "reps_list = [rep.text.replace('\\n\\n\\n\\n','').replace('      ',' ').strip().split('\\n') for rep in reps]\n",
    "reps_flat = [item for sublist in reps_list for item in sublist]\n",
    "reps_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['podgorskiy /',\n '',\n '',\n '',\n '      ALAE',\n 'timgrossmann /',\n '',\n '',\n '',\n '      InstaPy',\n '3b1b /',\n '',\n '',\n '',\n '      manim',\n 'TheAlgorithms /',\n '',\n '',\n '',\n '      Python',\n 'donnemartin /',\n '',\n '',\n '',\n '      system-design-primer',\n 'TachibanaYoshino /',\n '',\n '',\n '',\n '      AnimeGAN',\n 'alexgand /',\n '',\n '',\n '',\n '      springer_free_books',\n 'soimort /',\n '',\n '',\n '',\n '      you-get',\n 'scikit-learn /',\n '',\n '',\n '',\n '      scikit-learn',\n 'freqtrade /',\n '',\n '',\n '',\n '      freqtrade',\n 'NVlabs /',\n '',\n '',\n '',\n '      stylegan2',\n 'programthink /',\n '',\n '',\n '',\n '      zhao',\n 'volatilityfoundation /',\n '',\n '',\n '',\n '      volatility',\n 'Manisso /',\n '',\n '',\n '',\n '      fsociety',\n 'PyTorchLightning /',\n '',\n '',\n '',\n '      pytorch-lightning',\n 'OctoPrint /',\n '',\n '',\n '',\n '      OctoPrint',\n 'iperov /',\n '',\n '',\n '',\n '      DeepFaceLab',\n 'matplotlib /',\n '',\n '',\n '',\n '      mplfinance',\n 'NVIDIA /',\n '',\n '',\n '',\n '      apex',\n 'Alic-yuan /',\n '',\n '',\n '',\n '      nlp-beginner-finish',\n 'commaai /',\n '',\n '',\n '',\n '      comma10k',\n 'ycm-core /',\n '',\n '',\n '',\n '      YouCompleteMe',\n 'apprenticeharper /',\n '',\n '',\n '',\n '      DeDRM_tools',\n 'google-research /',\n '',\n '',\n '',\n '      bert',\n 'neha-chawla /',\n '',\n '',\n '',\n '      RandomPython']"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "#Trending Python Repositories in GitHub.\n",
    "reps = soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "reps_list = [rep.text.strip().split(\"\\n\") for rep in reps]\n",
    "reps_flat = [item for sublist in reps_list for item in sublist]\n",
    "\n",
    "reps_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-ac6cd911d7ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Trending Python repositories in GitHub.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'h3 lh-condensed'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreps_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreps_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreps_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-ac6cd911d7ab>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Trending Python repositories in GitHub.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'h3 lh-condensed'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreps_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m', '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mreps_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreps_list\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "#Trending Python repositories in GitHub.\n",
    "reps = soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "reps_list = [rep.text.replace('\\n\\n','\\n',', ').strip().split(\"\\n\") for rep in reps]\n",
    "reps_flat = [item for sublist in reps_list for item in sublist]\n",
    "\n",
    "reps_list\n",
    "#reps_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/4/44/The_Walt_Disney_Company_Logo.svg/120px-The_Walt_Disney_Company_Logo.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n '/static/images/wikimedia-button.png',\n '/static/images/poweredby_mediawiki_88x31.png']"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "#Image links from Walt Disney wikipedia page.\n",
    "imgs = [img.get('src') for img in soup.find_all('img')]\n",
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Souuuup III.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['#mw-head',\n '#p-search',\n 'https://en.wiktionary.org/wiki/Python',\n 'https://en.wiktionary.org/wiki/python',\n '#Snakes',\n '#Ancient_Greece',\n '#Media_and_entertainment',\n '#Computing',\n '#Engineering',\n '#Roller_coasters',\n '#Vehicles',\n '#Weaponry',\n '#People',\n '#Other_uses',\n '#See_also',\n '/w/index.php?title=Python&action=edit&section=1',\n '/wiki/Pythonidae',\n '/wiki/Python_(genus)',\n '/w/index.php?title=Python&action=edit&section=2',\n '/wiki/Python_(mythology)',\n '/wiki/Python_of_Aenus',\n '/wiki/Python_(painter)',\n '/wiki/Python_of_Byzantium',\n '/wiki/Python_of_Catana',\n '/w/index.php?title=Python&action=edit&section=3',\n '/wiki/Python_(film)',\n '/wiki/Pythons_2',\n '/wiki/Monty_Python',\n '/wiki/Python_(Monty)_Pictures',\n '/w/index.php?title=Python&action=edit&section=4',\n '/wiki/Python_(programming_language)',\n '/wiki/CPython',\n '/wiki/CMU_Common_Lisp',\n '/wiki/PERQ#PERQ_3',\n '/w/index.php?title=Python&action=edit&section=5',\n '/w/index.php?title=Python&action=edit&section=6',\n '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n '/wiki/Python_(Efteling)',\n '/w/index.php?title=Python&action=edit&section=7',\n '/wiki/Python_(automobile_maker)',\n '/wiki/Python_(Ford_prototype)',\n '/w/index.php?title=Python&action=edit&section=8',\n '/wiki/Colt_Python',\n '/wiki/Python_(missile)',\n '/wiki/Python_(nuclear_primary)',\n '/w/index.php?title=Python&action=edit&section=9',\n '/wiki/Python_Anghelo',\n '/w/index.php?title=Python&action=edit&section=10',\n '/wiki/PYTHON',\n '/w/index.php?title=Python&action=edit&section=11',\n '/wiki/Cython',\n '/wiki/Pyton',\n '/wiki/File:Disambig_gray.svg',\n '/wiki/Help:Disambiguation',\n 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n 'https://en.wikipedia.org/w/index.php?title=Python&oldid=943216744',\n '/wiki/Help:Category',\n '/wiki/Category:Disambiguation_pages',\n '/wiki/Category:Disambiguation_pages_with_short_description',\n '/wiki/Category:All_article_disambiguation_pages',\n '/wiki/Category:All_disambiguation_pages',\n '/wiki/Category:Animal_common_name_disambiguation_pages',\n '/wiki/Special:MyTalk',\n '/wiki/Special:MyContributions',\n '/w/index.php?title=Special:CreateAccount&returnto=Python',\n '/w/index.php?title=Special:UserLogin&returnto=Python',\n '/wiki/Python',\n '/wiki/Talk:Python',\n '/wiki/Python',\n '/w/index.php?title=Python&action=edit',\n '/w/index.php?title=Python&action=history',\n '/wiki/Main_Page',\n '/wiki/Main_Page',\n '/wiki/Wikipedia:Contents',\n '/wiki/Wikipedia:Featured_content',\n '/wiki/Portal:Current_events',\n '/wiki/Special:Random',\n 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n '//shop.wikimedia.org',\n '/wiki/Help:Contents',\n '/wiki/Wikipedia:About',\n '/wiki/Wikipedia:Community_portal',\n '/wiki/Special:RecentChanges',\n '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n '/wiki/Special:WhatLinksHere/Python',\n '/wiki/Special:RecentChangesLinked/Python',\n '/wiki/Wikipedia:File_Upload_Wizard',\n '/wiki/Special:SpecialPages',\n '/w/index.php?title=Python&oldid=943216744',\n '/w/index.php?title=Python&action=info',\n 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n '/w/index.php?title=Special:CiteThisPage&page=Python&id=943216744&wpFormIdentifier=titleform',\n 'https://commons.wikimedia.org/wiki/Category:Python',\n '/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Python',\n '/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen',\n '/w/index.php?title=Python&printable=yes',\n 'https://af.wikipedia.org/wiki/Python',\n 'https://als.wikipedia.org/wiki/Python',\n 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86',\n 'https://az.wikipedia.org/wiki/Python',\n 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n 'https://be.wikipedia.org/wiki/Python',\n 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n 'https://da.wikipedia.org/wiki/Python',\n 'https://de.wikipedia.org/wiki/Python',\n 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n 'https://fr.wikipedia.org/wiki/Python',\n 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n 'https://io.wikipedia.org/wiki/Pitono',\n 'https://id.wikipedia.org/wiki/Python',\n 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n 'https://lb.wikipedia.org/wiki/Python',\n 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n 'https://nl.wikipedia.org/wiki/Python',\n 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n 'https://no.wikipedia.org/wiki/Pyton',\n 'https://pl.wikipedia.org/wiki/Pyton',\n 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n 'https://sk.wikipedia.org/wiki/Python',\n 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n 'https://sh.wikipedia.org/wiki/Python',\n 'https://fi.wikipedia.org/wiki/Python',\n 'https://sv.wikipedia.org/wiki/Pyton',\n 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n 'https://tr.wikipedia.org/wiki/Python',\n 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n 'https://vi.wikipedia.org/wiki/Python',\n 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n '//creativecommons.org/licenses/by-sa/3.0/',\n '//foundation.wikimedia.org/wiki/Terms_of_Use',\n '//foundation.wikimedia.org/wiki/Privacy_policy',\n '//www.wikimediafoundation.org/',\n 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n '/wiki/Wikipedia:About',\n '/wiki/Wikipedia:General_disclaimer',\n '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n 'https://stats.wikimedia.org/#/en.wikipedia.org',\n 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n 'https://wikimediafoundation.org/',\n 'https://www.mediawiki.org/']"
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "#List of links on that page\n",
    "links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Soooooupp IV.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n"
    }
   ],
   "source": [
    "#Number of Titles that have changed in the United States Code since its last release point.\n",
    "ttls = soup.find_all('div',{'class':'usctitlechanged'})\n",
    "ttls_list = [ttl.text.replace('\\n\\n','\\n').strip().split(\"\\n\") for ttl in ttls]\n",
    "ttls_flat = [item for sublist in ttls_list for item in sublist]\n",
    "\n",
    "print(len(ttls_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Soup V.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['ALEJANDRO ROSALES CASTILLO',\n 'ARNOLDO JIMENEZ',\n 'JASON DEREK BROWN',\n 'YASER ABDEL SAID',\n 'ALEXIS FLORES',\n 'EUGENE PALMER',\n 'SANTIAGO VILLALBA MEDEROS',\n 'RAFAEL CARO-QUINTERO',\n 'ROBERT WILLIAM FISHER',\n 'BHADRESHKUMAR CHETANBHAI PATEL']"
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "names = soup.find_all('h3',{'class':'title'})\n",
    "names_list = [name.text.replace('\\n\\n','\\n').strip().split(\"\\n\") for name in names]\n",
    "names_flat = [item for sublist in names_list for item in sublist]\n",
    "\n",
    "names_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Sowppp VI.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Sowppp VII.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                 0  \\\n0       earthquake2020-05-03   06:56:51.009min ago   \n1       earthquake2020-05-03   06:38:34.627min ago   \n2       earthquake2020-05-03   06:18:46.547min ago   \n3                                                F   \n4   earthquake2020-05-03   05:46:20.11hr 19min ago   \n5   earthquake2020-05-03   05:41:43.41hr 24min ago   \n6   earthquake2020-05-03   05:39:17.01hr 26min ago   \n7   earthquake2020-05-03   05:27:51.91hr 38min ago   \n8   earthquake2020-05-03   05:27:33.21hr 38min ago   \n9   earthquake2020-05-03   05:27:29.51hr 38min ago   \n10  earthquake2020-05-03   05:21:01.01hr 45min ago   \n11  earthquake2020-05-03   05:19:50.61hr 46min ago   \n12  earthquake2020-05-03   05:18:12.01hr 47min ago   \n13  earthquake2020-05-03   05:17:47.41hr 48min ago   \n14  earthquake2020-05-03   04:57:23.22hr 08min ago   \n15                                               F   \n16  earthquake2020-05-03   04:51:30.82hr 14min ago   \n17                                               F   \n18                                               F   \n19  earthquake2020-05-03   04:38:58.02hr 27min ago   \n20  earthquake2020-05-03   04:23:42.12hr 42min ago   \n21  earthquake2020-05-03   04:20:11.12hr 45min ago   \n22  earthquake2020-05-03   03:48:20.03hr 17min ago   \n23                                               F   \n24  earthquake2020-05-03   03:31:52.13hr 34min ago   \n25  earthquake2020-05-03   03:30:20.33hr 35min ago   \n26                                              12   \n27  earthquake2020-05-03   03:17:32.43hr 48min ago   \n28  earthquake2020-05-03   03:05:11.04hr 00min ago   \n29  earthquake2020-05-03   02:51:32.04hr 14min ago   \n30                                               F   \n31  earthquake2020-05-03   02:33:12.64hr 32min ago   \n32  earthquake2020-05-03   02:26:33.94hr 39min ago   \n33                                              11   \n34  earthquake2020-05-03   02:05:17.15hr 00min ago   \n35  earthquake2020-05-03   01:34:19.25hr 31min ago   \n36  earthquake2020-05-03   01:17:40.55hr 48min ago   \n37  earthquake2020-05-03   01:13:21.05hr 52min ago   \n38  earthquake2020-05-03   01:02:58.86hr 03min ago   \n39  earthquake2020-05-03   00:58:54.46hr 07min ago   \n40                                              13   \n41  earthquake2020-05-03   00:38:09.96hr 27min ago   \n42  earthquake2020-05-03   00:35:07.96hr 30min ago   \n43  earthquake2020-05-03   00:16:41.06hr 49min ago   \n44                                              25   \n45                                               1   \n46  earthquake2020-05-03   00:07:35.06hr 58min ago   \n47                                            None   \n48                                            None   \n49                                            None   \n50                                             120   \n51                                               9   \n52  earthquake2020-05-02   23:40:15.97hr 25min ago   \n\n                                                 1  \\\n0                                             9.85   \n1                                            34.13   \n2                                            19.20   \n3   earthquake2020-05-03   05:54:41.91hr 11min ago   \n4                                            39.32   \n5                                            17.97   \n6                                            31.78   \n7                                            58.40   \n8                                            44.02   \n9                                            39.05   \n10                                            8.70   \n11                                           39.06   \n12                                            3.15   \n13                                           44.18   \n14                                           34.23   \n15  earthquake2020-05-03   04:54:53.02hr 11min ago   \n16                                           35.91   \n17  earthquake2020-05-03   04:50:29.62hr 15min ago   \n18  earthquake2020-05-03   04:41:05.62hr 24min ago   \n19                                           20.29   \n20                                           38.44   \n21                                           43.86   \n22                                            1.43   \n23  earthquake2020-05-03   03:35:55.03hr 30min ago   \n24                                           42.74   \n25                                           43.81   \n26                                              IV   \n27                                           39.27   \n28                                           28.68   \n29                                           51.41   \n30  earthquake2020-05-03   02:39:31.04hr 26min ago   \n31                                           37.86   \n32                                           35.82   \n33                                              IV   \n34                                           38.51   \n35                                           39.59   \n36                                           18.63   \n37                                            0.47   \n38                                           38.82   \n39                                           45.61   \n40                                              IV   \n41                                           34.06   \n42                                           33.47   \n43                                           36.80   \n44                                             III   \n45                                               F   \n46                                           37.33   \n47                                            None   \n48                                            None   \n49                                            None   \n50                                             III   \n51                                              IV   \n52                                           47.73   \n\n                                                 2       3      4       5  \\\n0                                                S  111.94      E      10   \n1                                                N   26.06      E       5   \n2                                                N  155.46      W      38   \n3                                            34.02       N  25.92       E   \n4                                                N   38.40      E       7   \n5                                                N   66.73      W      10   \n6                                                S   71.34      W      42   \n7                                                N  136.94      W      10   \n8                                                N  148.03      E      35   \n9                                                N   27.84      E       7   \n10                                               S  114.34      E      10   \n11                                               N   27.86      E      10   \n12                                               N  127.20      E      10   \n13                                               N  148.35      E      15   \n14                                               N   25.66      E       0   \n15                                           40.91       N  20.77       E   \n16                                               N   35.61      E       8   \n17                                           40.67       N  20.66       E   \n18                                           40.64       N  20.70       E   \n19                                               S   69.24      W      96   \n20                                               N   25.88      E       7   \n21                                               N  148.65      E      30   \n22                                               S  134.28      E      10   \n23                                           17.93       N  66.68       W   \n24                                               S  175.61      E       5   \n25                                               N   26.96      E      14   \n26  earthquake2020-05-03   03:24:40.23hr 41min ago   23.19      N  121.73   \n27                                               N   29.20      E       2   \n28                                               S  178.07      W     140   \n29                                               N   16.25      E       1   \n30                                           12.51       N  89.50       W   \n31                                               N   16.02      E      60   \n32                                               N   35.53      E      20   \n33  earthquake2020-05-03   02:08:56.74hr 57min ago   35.80      N   25.62   \n34                                               N   39.28      E       5   \n35                                               N   26.12      E       9   \n36                                               N   67.61      W      35   \n37                                               S  127.67      E      10   \n38                                               N  122.80      W       3   \n39                                               N   26.59      E      94   \n40  earthquake2020-05-03   00:44:24.46hr 21min ago   15.35      S   70.34   \n41                                               N   25.94      E      60   \n42                                               N  137.21      E     367   \n43                                               N    9.47      W      21   \n44  earthquake2020-05-03   00:16:15.26hr 49min ago   41.36      N   19.67   \n45  earthquake2020-05-03   00:10:39.36hr 55min ago   44.74      N   10.32   \n46                                               N  121.69      W       7   \n47                                            None    None   None    None   \n48                                            None    None   None    None   \n49                                            None    None   None    None   \n50  earthquake2020-05-02   23:49:14.27hr 16min ago   43.21      N   18.05   \n51  earthquake2020-05-02   23:44:35.67hr 21min ago   32.82      N  115.46   \n52                                               N    7.65      E      18   \n\n       6     7                                 8  \\\n0      M   3.8          SOUTH OF JAVA, INDONESIA   \n1     mb   4.2                     CRETE, GREECE   \n2     Md   2.2          ISLAND OF HAWAII, HAWAII   \n3     10    mb                               4.9   \n4     ML   2.2                    EASTERN TURKEY   \n5     ML   2.5                PUERTO RICO REGION   \n6     ML   3.6                   COQUIMBO, CHILE   \n7     ML   2.5               SOUTHEASTERN ALASKA   \n8     mb   4.4                     KURIL ISLANDS   \n9     ML   2.0                    WESTERN TURKEY   \n10     M   3.4            BALI REGION, INDONESIA   \n11    ML   2.6                    WESTERN TURKEY   \n12     M   3.9       KEPULAUAN TALAUD, INDONESIA   \n13    mb   4.5                     KURIL ISLANDS   \n14    ML   3.0                     CRETE, GREECE   \n15    25    ML                               2.2   \n16    ML   2.0           NEAR THE COAST OF SYRIA   \n17    10    ML                               2.1   \n18    15    ML                               2.3   \n19    ML   3.0                   TARAPACA, CHILE   \n20    ML   2.2                        AEGEAN SEA   \n21    mb   4.4             EAST OF KURIL ISLANDS   \n22     M   2.8  NEAR N COAST OF PAPUA, INDONESIA   \n23    10    ML                               2.9   \n24     M   4.1   OFF E. COAST OF S. ISLAND, N.Z.   \n25    ML   2.5                          BULGARIA   \n26     E    46                                mb   \n27    ML   2.7                    WESTERN TURKEY   \n28    mb   4.9           KERMADEC ISLANDS REGION   \n29    ML   3.0                            POLAND   \n30     9     M                               3.9   \n31    ML   2.9                        IONIAN SEA   \n32    ML   2.5           NEAR THE COAST OF SYRIA   \n33     E    67                                mb   \n34    ML   2.4                    EASTERN TURKEY   \n35    ML   2.2  NEAR THE COAST OF WESTERN TURKEY   \n36    ML   3.3                PUERTO RICO REGION   \n37     M   4.1              HALMAHERA, INDONESIA   \n38    Md   2.1               NORTHERN CALIFORNIA   \n39    ML   3.0                           ROMANIA   \n40     W   159                                mb   \n41    ML   3.4                     CRETE, GREECE   \n42    mb   4.0    NEAR S. COAST OF HONSHU, JAPAN   \n43    ML   2.1                 WEST OF GIBRALTAR   \n44     E     8                                ML   \n45     E     8                                ML   \n46    Md   2.0               NORTHERN CALIFORNIA   \n47  None  None                              None   \n48  None  None                              None   \n49  None  None                              None   \n50     E     0                                ML   \n51     W    10                                ML   \n52    ML   0.9      FRANCE-GERMANY BORDER REGION   \n\n                               9                      10                11  \n0               2020-05-03 07:05                    None              None  \n1               2020-05-03 07:01                    None              None  \n2               2020-05-03 06:22                    None              None  \n3                  CRETE, GREECE        2020-05-03 06:16              None  \n4               2020-05-03 05:54                    None              None  \n5               2020-05-03 06:38                    None              None  \n6               2020-05-03 06:20                    None              None  \n7               2020-05-03 06:00                    None              None  \n8               2020-05-03 05:51                    None              None  \n9               2020-05-03 05:32                    None              None  \n10              2020-05-03 05:35                    None              None  \n11              2020-05-03 05:28                    None              None  \n12              2020-05-03 05:40                    None              None  \n13              2020-05-03 06:33                    None              None  \n14              2020-05-03 05:00                    None              None  \n15   REPUBLIC OF NORTH MACEDONIA        2020-05-03 06:43              None  \n16              2020-05-03 05:19                    None              None  \n17                       ALBANIA        2020-05-03 06:26              None  \n18                       ALBANIA        2020-05-03 05:56              None  \n19              2020-05-03 04:52                    None              None  \n20              2020-05-03 05:18                    None              None  \n21              2020-05-03 05:41                    None              None  \n22              2020-05-03 04:00                    None              None  \n23            PUERTO RICO REGION        2020-05-03 04:09              None  \n24              2020-05-03 03:40                    None              None  \n25              2020-05-03 04:56                    None              None  \n26                           5.6                  TAIWAN  2020-05-03 03:42  \n27              2020-05-03 04:19                    None              None  \n28              2020-05-03 03:56                    None              None  \n29              2020-05-03 02:57                    None              None  \n30  OFF THE COAST OF EL SALVADOR        2020-05-03 03:03              None  \n31              2020-05-03 02:46                    None              None  \n32              2020-05-03 03:42                    None              None  \n33                           4.2           CRETE, GREECE  2020-05-03 06:23  \n34              2020-05-03 03:38                    None              None  \n35              2020-05-03 03:21                    None              None  \n36              2020-05-03 01:26                    None              None  \n37              2020-05-03 01:25                    None              None  \n38              2020-05-03 01:04                    None              None  \n39              2020-05-03 01:25                    None              None  \n40                           5.4           SOUTHERN PERU  2020-05-03 00:53  \n41              2020-05-03 03:05                    None              None  \n42              2020-05-03 02:06                    None              None  \n43              2020-05-03 00:20                    None              None  \n44                           2.5                 ALBANIA  2020-05-03 01:03  \n45                           2.8          NORTHERN ITALY  2020-05-03 00:26  \n46              2020-05-03 00:09                    None              None  \n47                          None                    None              None  \n48                          None                    None              None  \n49                          None                    None              None  \n50                           3.3  BOSNIA AND HERZEGOVINA  2020-05-03 04:29  \n51                           3.3     SOUTHERN CALIFORNIA  2020-05-02 23:57  \n52              2020-05-03 07:03                    None              None  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>earthquake2020-05-03   06:56:51.009min ago</td>\n      <td>9.85</td>\n      <td>S</td>\n      <td>111.94</td>\n      <td>E</td>\n      <td>10</td>\n      <td>M</td>\n      <td>3.8</td>\n      <td>SOUTH OF JAVA, INDONESIA</td>\n      <td>2020-05-03 07:05</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>earthquake2020-05-03   06:38:34.627min ago</td>\n      <td>34.13</td>\n      <td>N</td>\n      <td>26.06</td>\n      <td>E</td>\n      <td>5</td>\n      <td>mb</td>\n      <td>4.2</td>\n      <td>CRETE, GREECE</td>\n      <td>2020-05-03 07:01</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>earthquake2020-05-03   06:18:46.547min ago</td>\n      <td>19.20</td>\n      <td>N</td>\n      <td>155.46</td>\n      <td>W</td>\n      <td>38</td>\n      <td>Md</td>\n      <td>2.2</td>\n      <td>ISLAND OF HAWAII, HAWAII</td>\n      <td>2020-05-03 06:22</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>F</td>\n      <td>earthquake2020-05-03   05:54:41.91hr 11min ago</td>\n      <td>34.02</td>\n      <td>N</td>\n      <td>25.92</td>\n      <td>E</td>\n      <td>10</td>\n      <td>mb</td>\n      <td>4.9</td>\n      <td>CRETE, GREECE</td>\n      <td>2020-05-03 06:16</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>earthquake2020-05-03   05:46:20.11hr 19min ago</td>\n      <td>39.32</td>\n      <td>N</td>\n      <td>38.40</td>\n      <td>E</td>\n      <td>7</td>\n      <td>ML</td>\n      <td>2.2</td>\n      <td>EASTERN TURKEY</td>\n      <td>2020-05-03 05:54</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>earthquake2020-05-03   05:41:43.41hr 24min ago</td>\n      <td>17.97</td>\n      <td>N</td>\n      <td>66.73</td>\n      <td>W</td>\n      <td>10</td>\n      <td>ML</td>\n      <td>2.5</td>\n      <td>PUERTO RICO REGION</td>\n      <td>2020-05-03 06:38</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>earthquake2020-05-03   05:39:17.01hr 26min ago</td>\n      <td>31.78</td>\n      <td>S</td>\n      <td>71.34</td>\n      <td>W</td>\n      <td>42</td>\n      <td>ML</td>\n      <td>3.6</td>\n      <td>COQUIMBO, CHILE</td>\n      <td>2020-05-03 06:20</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>earthquake2020-05-03   05:27:51.91hr 38min ago</td>\n      <td>58.40</td>\n      <td>N</td>\n      <td>136.94</td>\n      <td>W</td>\n      <td>10</td>\n      <td>ML</td>\n      <td>2.5</td>\n      <td>SOUTHEASTERN ALASKA</td>\n      <td>2020-05-03 06:00</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>earthquake2020-05-03   05:27:33.21hr 38min ago</td>\n      <td>44.02</td>\n      <td>N</td>\n      <td>148.03</td>\n      <td>E</td>\n      <td>35</td>\n      <td>mb</td>\n      <td>4.4</td>\n      <td>KURIL ISLANDS</td>\n      <td>2020-05-03 05:51</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>earthquake2020-05-03   05:27:29.51hr 38min ago</td>\n      <td>39.05</td>\n      <td>N</td>\n      <td>27.84</td>\n      <td>E</td>\n      <td>7</td>\n      <td>ML</td>\n      <td>2.0</td>\n      <td>WESTERN TURKEY</td>\n      <td>2020-05-03 05:32</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>earthquake2020-05-03   05:21:01.01hr 45min ago</td>\n      <td>8.70</td>\n      <td>S</td>\n      <td>114.34</td>\n      <td>E</td>\n      <td>10</td>\n      <td>M</td>\n      <td>3.4</td>\n      <td>BALI REGION, INDONESIA</td>\n      <td>2020-05-03 05:35</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>earthquake2020-05-03   05:19:50.61hr 46min ago</td>\n      <td>39.06</td>\n      <td>N</td>\n      <td>27.86</td>\n      <td>E</td>\n      <td>10</td>\n      <td>ML</td>\n      <td>2.6</td>\n      <td>WESTERN TURKEY</td>\n      <td>2020-05-03 05:28</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>earthquake2020-05-03   05:18:12.01hr 47min ago</td>\n      <td>3.15</td>\n      <td>N</td>\n      <td>127.20</td>\n      <td>E</td>\n      <td>10</td>\n      <td>M</td>\n      <td>3.9</td>\n      <td>KEPULAUAN TALAUD, INDONESIA</td>\n      <td>2020-05-03 05:40</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>earthquake2020-05-03   05:17:47.41hr 48min ago</td>\n      <td>44.18</td>\n      <td>N</td>\n      <td>148.35</td>\n      <td>E</td>\n      <td>15</td>\n      <td>mb</td>\n      <td>4.5</td>\n      <td>KURIL ISLANDS</td>\n      <td>2020-05-03 06:33</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>earthquake2020-05-03   04:57:23.22hr 08min ago</td>\n      <td>34.23</td>\n      <td>N</td>\n      <td>25.66</td>\n      <td>E</td>\n      <td>0</td>\n      <td>ML</td>\n      <td>3.0</td>\n      <td>CRETE, GREECE</td>\n      <td>2020-05-03 05:00</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>F</td>\n      <td>earthquake2020-05-03   04:54:53.02hr 11min ago</td>\n      <td>40.91</td>\n      <td>N</td>\n      <td>20.77</td>\n      <td>E</td>\n      <td>25</td>\n      <td>ML</td>\n      <td>2.2</td>\n      <td>REPUBLIC OF NORTH MACEDONIA</td>\n      <td>2020-05-03 06:43</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>earthquake2020-05-03   04:51:30.82hr 14min ago</td>\n      <td>35.91</td>\n      <td>N</td>\n      <td>35.61</td>\n      <td>E</td>\n      <td>8</td>\n      <td>ML</td>\n      <td>2.0</td>\n      <td>NEAR THE COAST OF SYRIA</td>\n      <td>2020-05-03 05:19</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>F</td>\n      <td>earthquake2020-05-03   04:50:29.62hr 15min ago</td>\n      <td>40.67</td>\n      <td>N</td>\n      <td>20.66</td>\n      <td>E</td>\n      <td>10</td>\n      <td>ML</td>\n      <td>2.1</td>\n      <td>ALBANIA</td>\n      <td>2020-05-03 06:26</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>F</td>\n      <td>earthquake2020-05-03   04:41:05.62hr 24min ago</td>\n      <td>40.64</td>\n      <td>N</td>\n      <td>20.70</td>\n      <td>E</td>\n      <td>15</td>\n      <td>ML</td>\n      <td>2.3</td>\n      <td>ALBANIA</td>\n      <td>2020-05-03 05:56</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>earthquake2020-05-03   04:38:58.02hr 27min ago</td>\n      <td>20.29</td>\n      <td>S</td>\n      <td>69.24</td>\n      <td>W</td>\n      <td>96</td>\n      <td>ML</td>\n      <td>3.0</td>\n      <td>TARAPACA, CHILE</td>\n      <td>2020-05-03 04:52</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>earthquake2020-05-03   04:23:42.12hr 42min ago</td>\n      <td>38.44</td>\n      <td>N</td>\n      <td>25.88</td>\n      <td>E</td>\n      <td>7</td>\n      <td>ML</td>\n      <td>2.2</td>\n      <td>AEGEAN SEA</td>\n      <td>2020-05-03 05:18</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>earthquake2020-05-03   04:20:11.12hr 45min ago</td>\n      <td>43.86</td>\n      <td>N</td>\n      <td>148.65</td>\n      <td>E</td>\n      <td>30</td>\n      <td>mb</td>\n      <td>4.4</td>\n      <td>EAST OF KURIL ISLANDS</td>\n      <td>2020-05-03 05:41</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>earthquake2020-05-03   03:48:20.03hr 17min ago</td>\n      <td>1.43</td>\n      <td>S</td>\n      <td>134.28</td>\n      <td>E</td>\n      <td>10</td>\n      <td>M</td>\n      <td>2.8</td>\n      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n      <td>2020-05-03 04:00</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>F</td>\n      <td>earthquake2020-05-03   03:35:55.03hr 30min ago</td>\n      <td>17.93</td>\n      <td>N</td>\n      <td>66.68</td>\n      <td>W</td>\n      <td>10</td>\n      <td>ML</td>\n      <td>2.9</td>\n      <td>PUERTO RICO REGION</td>\n      <td>2020-05-03 04:09</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>earthquake2020-05-03   03:31:52.13hr 34min ago</td>\n      <td>42.74</td>\n      <td>S</td>\n      <td>175.61</td>\n      <td>E</td>\n      <td>5</td>\n      <td>M</td>\n      <td>4.1</td>\n      <td>OFF E. COAST OF S. ISLAND, N.Z.</td>\n      <td>2020-05-03 03:40</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>earthquake2020-05-03   03:30:20.33hr 35min ago</td>\n      <td>43.81</td>\n      <td>N</td>\n      <td>26.96</td>\n      <td>E</td>\n      <td>14</td>\n      <td>ML</td>\n      <td>2.5</td>\n      <td>BULGARIA</td>\n      <td>2020-05-03 04:56</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>12</td>\n      <td>IV</td>\n      <td>earthquake2020-05-03   03:24:40.23hr 41min ago</td>\n      <td>23.19</td>\n      <td>N</td>\n      <td>121.73</td>\n      <td>E</td>\n      <td>46</td>\n      <td>mb</td>\n      <td>5.6</td>\n      <td>TAIWAN</td>\n      <td>2020-05-03 03:42</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>earthquake2020-05-03   03:17:32.43hr 48min ago</td>\n      <td>39.27</td>\n      <td>N</td>\n      <td>29.20</td>\n      <td>E</td>\n      <td>2</td>\n      <td>ML</td>\n      <td>2.7</td>\n      <td>WESTERN TURKEY</td>\n      <td>2020-05-03 04:19</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>earthquake2020-05-03   03:05:11.04hr 00min ago</td>\n      <td>28.68</td>\n      <td>S</td>\n      <td>178.07</td>\n      <td>W</td>\n      <td>140</td>\n      <td>mb</td>\n      <td>4.9</td>\n      <td>KERMADEC ISLANDS REGION</td>\n      <td>2020-05-03 03:56</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>earthquake2020-05-03   02:51:32.04hr 14min ago</td>\n      <td>51.41</td>\n      <td>N</td>\n      <td>16.25</td>\n      <td>E</td>\n      <td>1</td>\n      <td>ML</td>\n      <td>3.0</td>\n      <td>POLAND</td>\n      <td>2020-05-03 02:57</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>F</td>\n      <td>earthquake2020-05-03   02:39:31.04hr 26min ago</td>\n      <td>12.51</td>\n      <td>N</td>\n      <td>89.50</td>\n      <td>W</td>\n      <td>9</td>\n      <td>M</td>\n      <td>3.9</td>\n      <td>OFF THE COAST OF EL SALVADOR</td>\n      <td>2020-05-03 03:03</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>earthquake2020-05-03   02:33:12.64hr 32min ago</td>\n      <td>37.86</td>\n      <td>N</td>\n      <td>16.02</td>\n      <td>E</td>\n      <td>60</td>\n      <td>ML</td>\n      <td>2.9</td>\n      <td>IONIAN SEA</td>\n      <td>2020-05-03 02:46</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>earthquake2020-05-03   02:26:33.94hr 39min ago</td>\n      <td>35.82</td>\n      <td>N</td>\n      <td>35.53</td>\n      <td>E</td>\n      <td>20</td>\n      <td>ML</td>\n      <td>2.5</td>\n      <td>NEAR THE COAST OF SYRIA</td>\n      <td>2020-05-03 03:42</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>11</td>\n      <td>IV</td>\n      <td>earthquake2020-05-03   02:08:56.74hr 57min ago</td>\n      <td>35.80</td>\n      <td>N</td>\n      <td>25.62</td>\n      <td>E</td>\n      <td>67</td>\n      <td>mb</td>\n      <td>4.2</td>\n      <td>CRETE, GREECE</td>\n      <td>2020-05-03 06:23</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>earthquake2020-05-03   02:05:17.15hr 00min ago</td>\n      <td>38.51</td>\n      <td>N</td>\n      <td>39.28</td>\n      <td>E</td>\n      <td>5</td>\n      <td>ML</td>\n      <td>2.4</td>\n      <td>EASTERN TURKEY</td>\n      <td>2020-05-03 03:38</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>earthquake2020-05-03   01:34:19.25hr 31min ago</td>\n      <td>39.59</td>\n      <td>N</td>\n      <td>26.12</td>\n      <td>E</td>\n      <td>9</td>\n      <td>ML</td>\n      <td>2.2</td>\n      <td>NEAR THE COAST OF WESTERN TURKEY</td>\n      <td>2020-05-03 03:21</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>earthquake2020-05-03   01:17:40.55hr 48min ago</td>\n      <td>18.63</td>\n      <td>N</td>\n      <td>67.61</td>\n      <td>W</td>\n      <td>35</td>\n      <td>ML</td>\n      <td>3.3</td>\n      <td>PUERTO RICO REGION</td>\n      <td>2020-05-03 01:26</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>earthquake2020-05-03   01:13:21.05hr 52min ago</td>\n      <td>0.47</td>\n      <td>S</td>\n      <td>127.67</td>\n      <td>E</td>\n      <td>10</td>\n      <td>M</td>\n      <td>4.1</td>\n      <td>HALMAHERA, INDONESIA</td>\n      <td>2020-05-03 01:25</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>earthquake2020-05-03   01:02:58.86hr 03min ago</td>\n      <td>38.82</td>\n      <td>N</td>\n      <td>122.80</td>\n      <td>W</td>\n      <td>3</td>\n      <td>Md</td>\n      <td>2.1</td>\n      <td>NORTHERN CALIFORNIA</td>\n      <td>2020-05-03 01:04</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>earthquake2020-05-03   00:58:54.46hr 07min ago</td>\n      <td>45.61</td>\n      <td>N</td>\n      <td>26.59</td>\n      <td>E</td>\n      <td>94</td>\n      <td>ML</td>\n      <td>3.0</td>\n      <td>ROMANIA</td>\n      <td>2020-05-03 01:25</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>13</td>\n      <td>IV</td>\n      <td>earthquake2020-05-03   00:44:24.46hr 21min ago</td>\n      <td>15.35</td>\n      <td>S</td>\n      <td>70.34</td>\n      <td>W</td>\n      <td>159</td>\n      <td>mb</td>\n      <td>5.4</td>\n      <td>SOUTHERN PERU</td>\n      <td>2020-05-03 00:53</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>earthquake2020-05-03   00:38:09.96hr 27min ago</td>\n      <td>34.06</td>\n      <td>N</td>\n      <td>25.94</td>\n      <td>E</td>\n      <td>60</td>\n      <td>ML</td>\n      <td>3.4</td>\n      <td>CRETE, GREECE</td>\n      <td>2020-05-03 03:05</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>earthquake2020-05-03   00:35:07.96hr 30min ago</td>\n      <td>33.47</td>\n      <td>N</td>\n      <td>137.21</td>\n      <td>E</td>\n      <td>367</td>\n      <td>mb</td>\n      <td>4.0</td>\n      <td>NEAR S. COAST OF HONSHU, JAPAN</td>\n      <td>2020-05-03 02:06</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>earthquake2020-05-03   00:16:41.06hr 49min ago</td>\n      <td>36.80</td>\n      <td>N</td>\n      <td>9.47</td>\n      <td>W</td>\n      <td>21</td>\n      <td>ML</td>\n      <td>2.1</td>\n      <td>WEST OF GIBRALTAR</td>\n      <td>2020-05-03 00:20</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>25</td>\n      <td>III</td>\n      <td>earthquake2020-05-03   00:16:15.26hr 49min ago</td>\n      <td>41.36</td>\n      <td>N</td>\n      <td>19.67</td>\n      <td>E</td>\n      <td>8</td>\n      <td>ML</td>\n      <td>2.5</td>\n      <td>ALBANIA</td>\n      <td>2020-05-03 01:03</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>1</td>\n      <td>F</td>\n      <td>earthquake2020-05-03   00:10:39.36hr 55min ago</td>\n      <td>44.74</td>\n      <td>N</td>\n      <td>10.32</td>\n      <td>E</td>\n      <td>8</td>\n      <td>ML</td>\n      <td>2.8</td>\n      <td>NORTHERN ITALY</td>\n      <td>2020-05-03 00:26</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>earthquake2020-05-03   00:07:35.06hr 58min ago</td>\n      <td>37.33</td>\n      <td>N</td>\n      <td>121.69</td>\n      <td>W</td>\n      <td>7</td>\n      <td>Md</td>\n      <td>2.0</td>\n      <td>NORTHERN CALIFORNIA</td>\n      <td>2020-05-03 00:09</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>120</td>\n      <td>III</td>\n      <td>earthquake2020-05-02   23:49:14.27hr 16min ago</td>\n      <td>43.21</td>\n      <td>N</td>\n      <td>18.05</td>\n      <td>E</td>\n      <td>0</td>\n      <td>ML</td>\n      <td>3.3</td>\n      <td>BOSNIA AND HERZEGOVINA</td>\n      <td>2020-05-03 04:29</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>9</td>\n      <td>IV</td>\n      <td>earthquake2020-05-02   23:44:35.67hr 21min ago</td>\n      <td>32.82</td>\n      <td>N</td>\n      <td>115.46</td>\n      <td>W</td>\n      <td>10</td>\n      <td>ML</td>\n      <td>3.3</td>\n      <td>SOUTHERN CALIFORNIA</td>\n      <td>2020-05-02 23:57</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>earthquake2020-05-02   23:40:15.97hr 25min ago</td>\n      <td>47.73</td>\n      <td>N</td>\n      <td>7.65</td>\n      <td>E</td>\n      <td>18</td>\n      <td>ML</td>\n      <td>0.9</td>\n      <td>FRANCE-GERMANY BORDER REGION</td>\n      <td>2020-05-03 07:03</td>\n      <td>None</td>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 317
    }
   ],
   "source": [
    "e_table = soup.find('tbody', {'id':'tbody'})\n",
    "rows = e_table.find_all('tr')\n",
    "\n",
    "data = []\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols if ele])\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Sowppp VII.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-249-7b947615be5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearthquake_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'table'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'haut_tableau'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mearthquake_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearthquake_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mearthquake_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 2081\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "earthquake_table = soup.find_all('table',{'id': 'haut_tableau'})\n",
    "earthquake_rows = earthquake_table.find_all('tr')\n",
    "earthquake_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              Event             Date               Location\n0       Hacklarious   May 9th - 10th  Everywhere, Worldwide\n1          RU Hacks  May 15th - 17th            Toronto, ON\n2    Hack the Chain  May 16th - 17th  Everywhere, Worldwide\n3      hths.hacks()  May 16th - 17th           Lincroft, NJ\n4           HackMTY  Aug 24th - 25th          Monterrey, MX\n..              ...              ...                    ...\n136         HackDSC  Apr 24th - 26th  Online, North America\n137        hack:now  Apr 24th - 26th           Berkeley, CA\n138   DistanceHacks    May 1st - 3rd           Millburn, NJ\n139    Hack at Home    May 2nd - 3rd  Everywhere, Worldwide\n140         TOHacks    May 2nd - 3rd            Toronto, ON\n\n[141 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Event</th>\n      <th>Date</th>\n      <th>Location</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hacklarious</td>\n      <td>May 9th - 10th</td>\n      <td>Everywhere, Worldwide</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>RU Hacks</td>\n      <td>May 15th - 17th</td>\n      <td>Toronto, ON</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hack the Chain</td>\n      <td>May 16th - 17th</td>\n      <td>Everywhere, Worldwide</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hths.hacks()</td>\n      <td>May 16th - 17th</td>\n      <td>Lincroft, NJ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HackMTY</td>\n      <td>Aug 24th - 25th</td>\n      <td>Monterrey, MX</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>HackDSC</td>\n      <td>Apr 24th - 26th</td>\n      <td>Online, North America</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>hack:now</td>\n      <td>Apr 24th - 26th</td>\n      <td>Berkeley, CA</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>DistanceHacks</td>\n      <td>May 1st - 3rd</td>\n      <td>Millburn, NJ</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>Hack at Home</td>\n      <td>May 2nd - 3rd</td>\n      <td>Everywhere, Worldwide</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>TOHacks</td>\n      <td>May 2nd - 3rd</td>\n      <td>Toronto, ON</td>\n    </tr>\n  </tbody>\n</table>\n<p>141 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 358
    }
   ],
   "source": [
    "#Event Names.\n",
    "event_names = soup.find_all('h3',{'class':'event-name'})\n",
    "en_list = [name.text.replace('\\n\\n','\\n').strip().split(\"\\n\") for name in event_names]\n",
    "en_flat = [item for sublist in en_list for item in sublist]\n",
    "\n",
    "#Event Dates.\n",
    "event_dates = soup.find_all('p',{'class':'event-date'})\n",
    "ed_list = [date.text.replace('\\n\\n','\\n').strip().split(\"\\n\") for date in event_dates]\n",
    "ed_flat = [item for sublist in ed_list for item in sublist]\n",
    "\n",
    "#Event Locations.\n",
    "event_locations = soup.find_all('div',{'class':'event-location'})\n",
    "el_list = [location.text.replace(',\\n          ', ', ').strip().split('\\n') for location in event_locations]\n",
    "el_flat = [item for sublist in el_list for item in sublist]\n",
    "\n",
    "#Unite everything into a list\n",
    "hackatons =  list(zip(en_flat, ed_flat, el_flat))\n",
    "\n",
    "#Show everything as a Pandas dataframe table. \n",
    "colnames = ['Event', 'Date', 'Location']\n",
    "data = hackatons\n",
    "\n",
    "hackatons_dataframe = pd.DataFrame(data, columns = colnames)\n",
    "hackatons_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-6f4ffae71d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Earthquakes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mearthquake_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'th'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'th2'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mearthquake_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearthquake_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mearthquake_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mearth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mearthquake_table\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mearthquake_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m         raise AttributeError(\n\u001b[0;32m-> 2081\u001b[0;31m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find_all'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "#Earthquakes.\n",
    "earthquake_table = soup.find_all('th',{'class':'th2'})\n",
    "earthquake_rows = earthquake_table.find_all('tr')\n",
    "earthquake_rows = [earth.text.replace('\\n\\n','\\n').strip().split(\"\\n\") for earth in earthquake_table]\n",
    "earthquake_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://mlh.io/seasons/na-2020/events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Sowppp VIII.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Casey has tweeted 25091 tweets.\n"
    }
   ],
   "source": [
    "#Let's try to do everyting by askig the User to input his/her Handle directly into the code. \n",
    "#Also, I'm using Casey Neistat's Handle for the stats: Casey\n",
    "\n",
    "handle = input('Input your account name on Twitter: ')\n",
    "temp = requests.get(url + handle)\n",
    "soup = BeautifulSoup(temp.text,'lxml')\n",
    "\n",
    "try:\n",
    "    tweet_box = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--tweets is-active'})\n",
    "    tweets = tweet_box.find('a').find('span',{'class':'ProfileNav-value'})\n",
    "    print(\"{} has tweeted {} tweets.\".format(handle, tweets.get('data-count')))\n",
    "\n",
    "except:\n",
    "    print('Account name not found...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Sowppp IX.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Casey has 2067861 followers.\n"
    }
   ],
   "source": [
    "#Again, in a bold move, I'll try to do everyting by askig the User to input his/her Handle directly into the code. \n",
    "#I'll use Casey's Handle again: Casey\n",
    "\n",
    "handle = input('Input your account name on Twitter: ')\n",
    "temp = requests.get(url + handle)\n",
    "soup = BeautifulSoup(temp.text,'lxml')\n",
    "\n",
    "try:\n",
    "    follow_box = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--followers'})\n",
    "    followers = follow_box.find('a').find('span',{'class':'ProfileNav-value'})\n",
    "    print(\"{} has {} followers.\".format(handle, followers.get('data-count')))\n",
    "except:\n",
    "    print('Account name not found...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Business and economy\nCrime and justice\nDefence\nEducation\nEnvironment\nGovernment\nGovernment spending\nHealth\nMapping\nSociety\nTowns and cities\nTransport\n"
    }
   ],
   "source": [
    "#Kinds of Datasets.\n",
    "table = soup.find_all('main',{'role':'main'})[0]\n",
    "each = table.find_all('a')\n",
    "for i in each:\n",
    "    print (i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['English6,066,000 articles',\n 'Español1,594,000 artículos',\n '日本語1,202,000 記事',\n 'Deutsch2,426,000 Artikel',\n 'Русский1,618,000 статей',\n 'Français2,207,000 articles',\n 'Italiano1,602,000 voci',\n '中文1,114,000 條目',\n 'Português1,029,000 artigos',\n 'Polski1,407,000 haseł']"
     },
     "metadata": {},
     "execution_count": 392
    }
   ],
   "source": [
    "#Language Names.\n",
    "lan_names = soup.find_all('div',{'class':'central-featured-lang'})\n",
    "lan_list = [lan.text.strip().replace('\\xa0',',').replace('\\n','').replace('+','').split(\"\\n\") for lan in lan_names]\n",
    "lan_flat = [item for sublist in lan_list for item in sublist]\n",
    "\n",
    "lan_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Sowppp XI.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Rank                            Language Speakers  \\\n1     1                    Mandarin Chinese      918   \n2     2                             Spanish      480   \n3     3                             English      379   \n4     4  Hindi (Sanskritised Hindustani)[9]      341   \n5     5                             Bengali      228   \n6     6                          Portuguese      221   \n7     7                             Russian      154   \n8     8                            Japanese      128   \n9     9                 Western Punjabi[10]     92.7   \n10   10                             Marathi     83.1   \n\n   Percentage of the World Population            Language Family  \n1                              11.922        Sino-TibetanSinitic  \n2                               5.994       Indo-EuropeanRomance  \n3                               4.922      Indo-EuropeanGermanic  \n4                               4.429    Indo-EuropeanIndo-Aryan  \n5                               2.961    Indo-EuropeanIndo-Aryan  \n6                               2.870       Indo-EuropeanRomance  \n7                               2.000  Indo-EuropeanBalto-Slavic  \n8                               1.662            JaponicJapanese  \n9                               1.204    Indo-EuropeanIndo-Aryan  \n10                              1.079    Indo-EuropeanIndo-Aryan  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rank</th>\n      <th>Language</th>\n      <th>Speakers</th>\n      <th>Percentage of the World Population</th>\n      <th>Language Family</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Mandarin Chinese</td>\n      <td>918</td>\n      <td>11.922</td>\n      <td>Sino-TibetanSinitic</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Spanish</td>\n      <td>480</td>\n      <td>5.994</td>\n      <td>Indo-EuropeanRomance</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>English</td>\n      <td>379</td>\n      <td>4.922</td>\n      <td>Indo-EuropeanGermanic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Hindi (Sanskritised Hindustani)[9]</td>\n      <td>341</td>\n      <td>4.429</td>\n      <td>Indo-EuropeanIndo-Aryan</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>Bengali</td>\n      <td>228</td>\n      <td>2.961</td>\n      <td>Indo-EuropeanIndo-Aryan</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>Portuguese</td>\n      <td>221</td>\n      <td>2.870</td>\n      <td>Indo-EuropeanRomance</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>Russian</td>\n      <td>154</td>\n      <td>2.000</td>\n      <td>Indo-EuropeanBalto-Slavic</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>Japanese</td>\n      <td>128</td>\n      <td>1.662</td>\n      <td>JaponicJapanese</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>Western Punjabi[10]</td>\n      <td>92.7</td>\n      <td>1.204</td>\n      <td>Indo-EuropeanIndo-Aryan</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>Marathi</td>\n      <td>83.1</td>\n      <td>1.079</td>\n      <td>Indo-EuropeanIndo-Aryan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 593
    }
   ],
   "source": [
    "#Define the Table\n",
    "data = []\n",
    "table_body = soup.find('tbody')\n",
    "\n",
    "#Define the Rows\n",
    "rows = table_body.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]\n",
    "    data.append([ele for ele in cols if ele])\n",
    "\n",
    "#Building the Data Frame. \n",
    "colnames = ['Rank', 'Language', 'Speakers', 'Percentage of the World Population', 'Language Family']\n",
    "languages_dataframe = pd.DataFrame(data, columns = colnames)\n",
    "languages_dataframe[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-553-bf96c35f58aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mlanguages_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "#Laguages.\n",
    "languages = []\n",
    "for row in soup.select('tbody tr'):\n",
    "    row_text = [x.text for x in row.find_all('td')]\n",
    "    languages.append(row_text)\n",
    "\n",
    "languages_rows = languages.replace(\"\\n\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting and Making the Sowppp X.\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37664bitanaconda3virtualenv0697af1ee67a458e9253591065064715"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}