{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "b'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-sf1Ppoo8CVx+YA47iIi2IUMYtjH17Errad0dPA+lo7DVd6VW1Mdy+TBcpD06Z6FN8MKI7TH5fpYU33+DhP25kg==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-b1fd4fa68'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "#your code\n",
    "html = requests.get(url).content\n",
    "html[:600]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Armani Ferrante',\n",
       " 'Julien Le Coupanec',\n",
       " 'Anton Medvedev',\n",
       " 'Kyle Conroy',\n",
       " 'mattn',\n",
       " 'Bartlomiej Plotka',\n",
       " 'Will McGugan',\n",
       " 'Stephan Dilly',\n",
       " 'Juliette',\n",
       " '陈帅',\n",
       " 'Andrey Sitnik',\n",
       " 'Marc Rousavy',\n",
       " 'Josh Bleecher Snyder',\n",
       " 'Ritchie Vink',\n",
       " 'maiyang',\n",
       " 'David Pedersen',\n",
       " 'Kenny Kerr',\n",
       " 'Ha Thach',\n",
       " 'Arvid Norberg',\n",
       " 'Simon Ser',\n",
       " 'Rich Harris',\n",
       " 'Vishal Rajput',\n",
       " 'Hongbo Zhang',\n",
       " 'Paweł Spychalski',\n",
       " 'PySimpleGUI']"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "class_ = soup.select('h1[class=\"h3 lh-condensed\"] ')\n",
    "class_clean = [rev.text for rev in class_]\n",
    "class_clean = [element.strip().replace(\"\\n\",\"\") for element in  class_clean]\n",
    "class_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "b'\\n\\n<!DOCTYPE html>\\n<html lang=\"en\" >\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dn'"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "html = requests.get(url).content\n",
    "html[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html,'lxml')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['donnemartin /      system-design-primer',\n",
       " 'pyston /      pyston',\n",
       " 'pallupz /      covid-vaccine-booking',\n",
       " 'hellerve /      programming-talks',\n",
       " 'ericaltendorf /      plotman',\n",
       " 'fighting41love /      funNLP',\n",
       " 'Chia-Network /      chia-blockchain',\n",
       " 'd2l-ai /      d2l-en',\n",
       " 'TheAlgorithms /      Python',\n",
       " 'Mukosame /      Anime2Sketch',\n",
       " 'swar /      Swar-Chia-Plot-Manager',\n",
       " 'Z4nzu /      hackingtool',\n",
       " 'Rog3rSm1th /      Profil3r',\n",
       " 'NVlabs /      stylegan2',\n",
       " 'httpie /      httpie',\n",
       " 'lucidrains /      mlp-mixer-pytorch',\n",
       " '3b1b /      manim',\n",
       " 'msxie92 /      MangaRestoration',\n",
       " 'opsdisk /      pagodo',\n",
       " 'ycm-core /      YouCompleteMe',\n",
       " 'MisterHW /      44780HD',\n",
       " 'leeyiding /      get_CCB',\n",
       " 'Binance-docs /      Binance_Futures_python',\n",
       " 'wbt5 /      real-url',\n",
       " 'donnemartin /      interactive-coding-challenges']"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "class_ = soup.select('h1[class=\"h3 lh-condensed\"] ')\n",
    "class_clean = [rev.text for rev in class_]\n",
    "class_clean = [element.strip().replace(\"\\n\",\"\") for element in  class_clean]\n",
    "class_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://github.githubassets.com/images/search-key-slash.svg\n\n\n\n\nhttps://avatars.githubusercontent.com/u/5458997?s=40&v=4\nhttps://avatars.githubusercontent.com/u/171818?s=40&v=4\nhttps://avatars.githubusercontent.com/u/7440735?s=40&v=4\nhttps://avatars.githubusercontent.com/u/8622362?s=40&v=4\nhttps://avatars.githubusercontent.com/u/6084440?s=40&v=4\nhttps://avatars.githubusercontent.com/u/2894642?s=40&v=4\nhttps://avatars.githubusercontent.com/u/219470?s=40&v=4\nhttps://avatars.githubusercontent.com/u/144359?s=40&v=4\nhttps://avatars.githubusercontent.com/u/588792?s=40&v=4\nhttps://avatars.githubusercontent.com/u/1623689?s=40&v=4\nhttps://avatars.githubusercontent.com/u/63504047?s=40&v=4\nhttps://avatars.githubusercontent.com/in/15368?s=40&v=4\nhttps://avatars.githubusercontent.com/u/49510780?s=40&v=4\nhttps://avatars.githubusercontent.com/u/22151336?s=40&v=4\nhttps://avatars.githubusercontent.com/u/514419?s=40&v=4\nhttps://avatars.githubusercontent.com/u/7725188?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3238748?s=40&v=4\nhttps://avatars.githubusercontent.com/u/24766695?s=40&v=4\nhttps://avatars.githubusercontent.com/u/7789327?s=40&v=4\nhttps://avatars.githubusercontent.com/u/4299398?s=40&v=4\nhttps://avatars.githubusercontent.com/u/73372694?s=40&v=4\nhttps://avatars.githubusercontent.com/u/543719?s=40&v=4\nhttps://avatars.githubusercontent.com/u/31668498?s=40&v=4\nhttps://avatars.githubusercontent.com/u/30672182?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3326247?s=40&v=4\nhttps://avatars.githubusercontent.com/u/11475294?s=40&v=4\nhttps://avatars.githubusercontent.com/u/13541953?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3929807?s=40&v=4\nhttps://avatars.githubusercontent.com/u/30624750?s=40&v=4\nhttps://avatars.githubusercontent.com/u/18108713?s=40&v=4\nhttps://avatars.githubusercontent.com/u/7254853?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3069354?s=40&v=4\nhttps://avatars.githubusercontent.com/u/30377676?s=40&v=4\nhttps://avatars.githubusercontent.com/u/5336?s=40&v=4\nhttps://avatars.githubusercontent.com/u/25171429?s=40&v=4\nhttps://avatars.githubusercontent.com/u/22279212?s=40&v=4\nhttps://avatars.githubusercontent.com/u/421857?s=40&v=4\nhttps://avatars.githubusercontent.com/u/4139355?s=40&v=4\nhttps://avatars.githubusercontent.com/u/23621655?s=40&v=4\nhttps://avatars.githubusercontent.com/u/2390222?s=40&v=4\nhttps://avatars.githubusercontent.com/u/14276147?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3709715?s=40&v=4\nhttps://avatars.githubusercontent.com/u/14369357?s=40&v=4\nhttps://avatars.githubusercontent.com/u/67177269?s=40&v=4\nhttps://avatars.githubusercontent.com/u/10573038?s=40&v=4\nhttps://avatars.githubusercontent.com/u/6279720?s=40&v=4\nhttps://avatars.githubusercontent.com/u/14857247?s=40&v=4\nhttps://avatars.githubusercontent.com/u/53619236?s=40&v=4\nhttps://avatars.githubusercontent.com/u/75458290?s=40&v=4\nhttps://avatars.githubusercontent.com/u/12022115?s=40&v=4\nhttps://avatars.githubusercontent.com/u/15592?s=40&v=4\nhttps://avatars.githubusercontent.com/u/25708027?s=40&v=4\nhttps://avatars.githubusercontent.com/u/66028078?s=40&v=4\nhttps://avatars.githubusercontent.com/u/59213927?s=40&v=4\nhttps://avatars.githubusercontent.com/u/33063403?s=40&v=4\nhttps://avatars.githubusercontent.com/u/48095937?s=40&v=4\nhttps://avatars.githubusercontent.com/u/75535027?s=40&v=4\nhttps://avatars.githubusercontent.com/u/2840348?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3089181?s=40&v=4\nhttps://avatars.githubusercontent.com/u/326885?s=40&v=4\nhttps://avatars.githubusercontent.com/u/305268?s=40&v=4\nhttps://avatars.githubusercontent.com/u/61052993?s=40&v=4\nhttps://avatars.githubusercontent.com/u/424045?s=40&v=4\nhttps://avatars.githubusercontent.com/u/203572?s=40&v=4\nhttps://avatars.githubusercontent.com/u/108653?s=40&v=4\nhttps://avatars.githubusercontent.com/u/6318811?s=40&v=4\nhttps://avatars.githubusercontent.com/u/11601040?s=40&v=4\nhttps://avatars.githubusercontent.com/u/13440601?s=40&v=4\nhttps://avatars.githubusercontent.com/u/43117506?s=40&v=4\nhttps://avatars.githubusercontent.com/u/44120331?s=40&v=4\nhttps://avatars.githubusercontent.com/u/35234358?s=40&v=4\nhttps://avatars.githubusercontent.com/u/34064199?s=40&v=4\nhttps://avatars.githubusercontent.com/u/8660252?s=40&v=4\nhttps://avatars.githubusercontent.com/u/10263598?s=40&v=4\nhttps://avatars.githubusercontent.com/u/64081845?s=40&v=4\nhttps://avatars.githubusercontent.com/u/919444?s=40&v=4\nhttps://avatars.githubusercontent.com/u/10026824?s=40&v=4\nhttps://avatars.githubusercontent.com/u/10212162?s=40&v=4\nhttps://avatars.githubusercontent.com/u/28640366?s=40&v=4\nhttps://avatars.githubusercontent.com/u/10584846?s=40&v=4\nhttps://avatars.githubusercontent.com/u/24977335?s=40&v=4\nhttps://avatars.githubusercontent.com/u/49098278?s=40&v=4\nhttps://avatars.githubusercontent.com/u/14211550?s=40&v=4\nhttps://avatars.githubusercontent.com/u/4384056?s=40&v=4\nhttps://avatars.githubusercontent.com/u/53783973?s=40&v=4\nhttps://avatars.githubusercontent.com/u/365354?s=40&v=4\nhttps://avatars.githubusercontent.com/u/31032561?s=40&v=4\nhttps://avatars.githubusercontent.com/u/379309?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3039195?s=40&v=4\nhttps://avatars.githubusercontent.com/u/9410067?s=40&v=4\nhttps://avatars.githubusercontent.com/u/23449?s=40&v=4\nhttps://avatars.githubusercontent.com/in/29110?s=40&v=4\nhttps://avatars.githubusercontent.com/u/30204608?s=40&v=4\nhttps://avatars.githubusercontent.com/u/5458997?s=40&v=4\nhttps://avatars.githubusercontent.com/u/3220235?s=40&v=4\nhttps://avatars.githubusercontent.com/u/10473142?s=40&v=4\nhttps://avatars.githubusercontent.com/u/12061835?s=40&v=4\nhttps://avatars.githubusercontent.com/u/1276131?s=40&v=4\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('img'):\n",
    "        print(link.get('src'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#mw-head\n#searchInput\nhttps://en.wiktionary.org/wiki/Python\nhttps://en.wiktionary.org/wiki/python\n/wiki/Pythons\n/wiki/Python_(genus)\n#Computing\n#People\n#Roller_coasters\n#Vehicles\n#Weaponry\n#Other_uses\n#See_also\n/w/index.php?title=Python&action=edit&section=1\n/wiki/Python_(programming_language)\n/wiki/CMU_Common_Lisp\n/wiki/PERQ#PERQ_3\n/w/index.php?title=Python&action=edit&section=2\n/wiki/Python_of_Aenus\n/wiki/Python_(painter)\n/wiki/Python_of_Byzantium\n/wiki/Python_of_Catana\n/wiki/Python_Anghelo\n/w/index.php?title=Python&action=edit&section=3\n/wiki/Python_(Efteling)\n/wiki/Python_(Busch_Gardens_Tampa_Bay)\n/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)\n/w/index.php?title=Python&action=edit&section=4\n/wiki/Python_(automobile_maker)\n/wiki/Python_(Ford_prototype)\n/w/index.php?title=Python&action=edit&section=5\n/wiki/Python_(missile)\n/wiki/Python_(nuclear_primary)\n/wiki/Colt_Python\n/w/index.php?title=Python&action=edit&section=6\n/wiki/PYTHON\n/wiki/Python_(film)\n/wiki/Python_(mythology)\n/wiki/Monty_Python\n/wiki/Python_(Monty)_Pictures\n/w/index.php?title=Python&action=edit&section=7\n/wiki/Cython\n/wiki/Pyton\n/wiki/Pithon\n/wiki/File:Disambig_gray.svg\n/wiki/Help:Disambiguation\nhttps://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0\nhttps://en.wikipedia.org/w/index.php?title=Python&oldid=997582414\n/wiki/Help:Category\n/wiki/Category:Disambiguation_pages\n/wiki/Category:Human_name_disambiguation_pages\n/wiki/Category:Disambiguation_pages_with_given-name-holder_lists\n/wiki/Category:Disambiguation_pages_with_short_descriptions\n/wiki/Category:Short_description_is_different_from_Wikidata\n/wiki/Category:All_article_disambiguation_pages\n/wiki/Category:All_disambiguation_pages\n/wiki/Category:Animal_common_name_disambiguation_pages\n/wiki/Special:MyTalk\n/wiki/Special:MyContributions\n/w/index.php?title=Special:CreateAccount&returnto=Python\n/w/index.php?title=Special:UserLogin&returnto=Python\n/wiki/Python\n/wiki/Talk:Python\n/wiki/Python\n/w/index.php?title=Python&action=edit\n/w/index.php?title=Python&action=history\n/wiki/Main_Page\n/wiki/Main_Page\n/wiki/Wikipedia:Contents\n/wiki/Portal:Current_events\n/wiki/Special:Random\n/wiki/Wikipedia:About\n//en.wikipedia.org/wiki/Wikipedia:Contact_us\nhttps://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n/wiki/Help:Contents\n/wiki/Help:Introduction\n/wiki/Wikipedia:Community_portal\n/wiki/Special:RecentChanges\n/wiki/Wikipedia:File_Upload_Wizard\n/wiki/Special:WhatLinksHere/Python\n/wiki/Special:RecentChangesLinked/Python\n/wiki/Wikipedia:File_Upload_Wizard\n/wiki/Special:SpecialPages\n/w/index.php?title=Python&oldid=997582414\n/w/index.php?title=Python&action=info\n/w/index.php?title=Special:CiteThisPage&page=Python&id=997582414&wpFormIdentifier=titleform\nhttps://www.wikidata.org/wiki/Special:EntityPage/Q747452\n/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen\n/w/index.php?title=Python&printable=yes\nhttps://commons.wikimedia.org/wiki/Category:Python\nhttps://af.wikipedia.org/wiki/Python\nhttps://als.wikipedia.org/wiki/Python\nhttps://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)\nhttps://az.wikipedia.org/wiki/Python\nhttps://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)\nhttps://be.wikipedia.org/wiki/Python\nhttps://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)\nhttps://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)\nhttps://da.wikipedia.org/wiki/Python\nhttps://de.wikipedia.org/wiki/Python\nhttps://eo.wikipedia.org/wiki/Pitono_(apartigilo)\nhttps://eu.wikipedia.org/wiki/Python_(argipena)\nhttps://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86\nhttps://fr.wikipedia.org/wiki/Python\nhttps://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0\nhttps://hr.wikipedia.org/wiki/Python_(razdvojba)\nhttps://io.wikipedia.org/wiki/Pitono\nhttps://id.wikipedia.org/wiki/Python\nhttps://ia.wikipedia.org/wiki/Python_(disambiguation)\nhttps://is.wikipedia.org/wiki/Python_(a%C3%B0greining)\nhttps://it.wikipedia.org/wiki/Python_(disambigua)\nhttps://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F\nhttps://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)\nhttps://kg.wikipedia.org/wiki/Mboma_(nyoka)\nhttps://la.wikipedia.org/wiki/Python_(discretiva)\nhttps://lb.wikipedia.org/wiki/Python\nhttps://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)\nhttps://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)\nhttps://nl.wikipedia.org/wiki/Python\nhttps://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3\nhttps://no.wikipedia.org/wiki/Pyton\nhttps://pl.wikipedia.org/wiki/Pyton\nhttps://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)\nhttps://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)\nhttps://sk.wikipedia.org/wiki/Python\nhttps://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)\nhttps://sh.wikipedia.org/wiki/Python\nhttps://fi.wikipedia.org/wiki/Python\nhttps://sv.wikipedia.org/wiki/Pyton\nhttps://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99\nhttps://tr.wikipedia.org/wiki/Python\nhttps://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD\nhttps://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86\nhttps://vi.wikipedia.org/wiki/Python\nhttps://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)\nhttps://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia\n//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n//creativecommons.org/licenses/by-sa/3.0/\n//foundation.wikimedia.org/wiki/Terms_of_Use\n//foundation.wikimedia.org/wiki/Privacy_policy\n//www.wikimediafoundation.org/\nhttps://foundation.wikimedia.org/wiki/Privacy_policy\n/wiki/Wikipedia:About\n/wiki/Wikipedia:General_disclaimer\n//en.wikipedia.org/wiki/Wikipedia:Contact_us\n//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile\nhttps://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\nhttps://stats.wikimedia.org/#/en.wikipedia.org\nhttps://foundation.wikimedia.org/wiki/Cookie_statement\nhttps://wikimediafoundation.org/\nhttps://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "links = soup.find_all('a', href=True)\n",
    "for link in links:\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "metadata": {},
     "execution_count": 120
    }
   ],
   "source": [
    "#your code\n",
    "class_ = soup.find_all('div', class_='usctitle')\n",
    "class_ = class_[-1].text.strip().replace(\"\\n\",\"\")\n",
    "class_ = re.findall('\\d+', class_)\n",
    "int(class_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "source": [
    "#your code \n",
    "class_ = soup.select('h3[class=\"title\"] ')\n",
    "class_clean = [rev.text for rev in class_]\n",
    "class_clean = [element.strip().replace(\"\\n\",\"\") for element in  class_clean]\n",
    "class_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "import lxml.html as lh\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html = requests.get(url)\n",
    "soup = lh.fromstring(html.content)\n",
    "rows = soup.xpath('//tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<Element tr at 0x1f8e54ba590>,\n",
       " <Element tr at 0x1f8e54ba9f0>,\n",
       " <Element tr at 0x1f8e54baa40>,\n",
       " <Element tr at 0x1f8e54babd0>,\n",
       " <Element tr at 0x1f8e54bacc0>,\n",
       " <Element tr at 0x1f8e54bad10>,\n",
       " <Element tr at 0x1f8e54bad60>,\n",
       " <Element tr at 0x1f8e54badb0>,\n",
       " <Element tr at 0x1f8e54bae00>,\n",
       " <Element tr at 0x1f8e54bae50>,\n",
       " <Element tr at 0x1f8e54baea0>,\n",
       " <Element tr at 0x1f8e54baef0>,\n",
       " <Element tr at 0x1f8e54baf40>,\n",
       " <Element tr at 0x1f8e54baf90>,\n",
       " <Element tr at 0x1f8e54d0040>,\n",
       " <Element tr at 0x1f8e54d0090>,\n",
       " <Element tr at 0x1f8e54d00e0>,\n",
       " <Element tr at 0x1f8e54d0130>,\n",
       " <Element tr at 0x1f8e54d0180>,\n",
       " <Element tr at 0x1f8e54d01d0>,\n",
       " <Element tr at 0x1f8e54d0220>,\n",
       " <Element tr at 0x1f8e54d0270>,\n",
       " <Element tr at 0x1f8e54d02c0>,\n",
       " <Element tr at 0x1f8e54d0310>,\n",
       " <Element tr at 0x1f8e54d0360>,\n",
       " <Element tr at 0x1f8e54d03b0>,\n",
       " <Element tr at 0x1f8e54d0400>,\n",
       " <Element tr at 0x1f8e54d0450>,\n",
       " <Element tr at 0x1f8e54d04a0>,\n",
       " <Element tr at 0x1f8e54d04f0>,\n",
       " <Element tr at 0x1f8e54d0540>,\n",
       " <Element tr at 0x1f8e54d0590>,\n",
       " <Element tr at 0x1f8e54d05e0>,\n",
       " <Element tr at 0x1f8e54d0630>,\n",
       " <Element tr at 0x1f8e54d0680>,\n",
       " <Element tr at 0x1f8e54d06d0>,\n",
       " <Element tr at 0x1f8e54d0720>,\n",
       " <Element tr at 0x1f8e54d0770>,\n",
       " <Element tr at 0x1f8e54d07c0>,\n",
       " <Element tr at 0x1f8e54d0810>,\n",
       " <Element tr at 0x1f8e54d0860>,\n",
       " <Element tr at 0x1f8e54d08b0>,\n",
       " <Element tr at 0x1f8e54d0900>,\n",
       " <Element tr at 0x1f8e54d0950>,\n",
       " <Element tr at 0x1f8e54d09a0>,\n",
       " <Element tr at 0x1f8e54d09f0>,\n",
       " <Element tr at 0x1f8e54d0a40>,\n",
       " <Element tr at 0x1f8e54d0a90>,\n",
       " <Element tr at 0x1f8e54d0ae0>,\n",
       " <Element tr at 0x1f8e54d0b30>]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#[len(row) for row in rows]\n",
    "earthq = [row for row in rows if len(row)==13]\n",
    "earthq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '2021-05-1015:20:22.712min ago',\n",
       " '28.19',\n",
       " 'N',\n",
       " '16.39',\n",
       " 'W',\n",
       " '19',\n",
       " 'ML',\n",
       " '1.7',\n",
       " 'CANARY ISLANDS, SPAIN REGION',\n",
       " '2021-05-10 15:29']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "rows = []\n",
    "for row in earthq:\n",
    "    if len(row)==13:\n",
    "        cols = []\n",
    "        for col in row:\n",
    "            dato = col.text_content()\n",
    "            cols.append(dato.strip().replace('\\xa0','').replace('earthquake',''))\n",
    "    rows.append(cols)\n",
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    row[1] = row[3][:10]\n",
    "    row[2] = row[3][10:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df = df[[1,2,4,6,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          date      time latitud longitud                       region name\n0   2021-05-10  15:20:22   28.19    16.39      CANARY ISLANDS, SPAIN REGION\n1   2021-05-10  14:40:40    2.47    78.95                           ECUADOR\n2   2021-05-10  14:36:02   13.96   144.90                       GUAM REGION\n3   2021-05-10  14:13:54   45.71    26.60                           ROMANIA\n4   2021-05-10  14:10:20   29.22   176.12           KERMADEC ISLANDS REGION\n5   2021-05-10  14:08:41   36.39    27.10  DODECANESE IS.-TURKEY BORDER REG\n6   2021-05-10  13:46:00    5.25   125.32             MINDANAO, PHILIPPINES\n7   2021-05-10  13:41:23   16.50    95.31                    OAXACA, MEXICO\n8   2021-05-10  13:00:30    8.93    84.17           OFF COAST OF COSTA RICA\n9   2021-05-10  12:53:41    2.48    78.98                           ECUADOR\n10  2021-05-10  12:32:52   38.19   117.86                            NEVADA\n11  2021-05-10  12:26:31    4.72    97.94       NORTHERN SUMATRA, INDONESIA\n12  2021-05-10  12:20:57   19.24   155.37          ISLAND OF HAWAII, HAWAII\n13  2021-05-10  12:17:16   19.20   155.42          ISLAND OF HAWAII, HAWAII\n14  2021-05-10  12:16:27   19.12    69.43         DOMINICAN REPUBLIC REGION\n15  2021-05-10  11:58:18   19.21   155.42          ISLAND OF HAWAII, HAWAII\n16  2021-05-10  11:52:30   24.12    69.92                ANTOFAGASTA, CHILE\n17  2021-05-10  11:50:32   18.36    69.52         DOMINICAN REPUBLIC REGION\n18  2021-05-10  11:48:37   31.21    70.37               SAN JUAN, ARGENTINA\n19  2021-05-10  11:47:38   23.86    67.02                  JUJUY, ARGENTINA\n20  2021-05-10  11:46:49   32.67    68.75                MENDOZA, ARGENTINA\n21  2021-05-10  11:43:03   33.38   179.54         SOUTH OF KERMADEC ISLANDS\n22  2021-05-10  11:20:02   19.85   120.45   BABUYAN ISL REGION, PHILIPPINES\n23  2021-05-10  11:10:02   32.48   137.57         IZU ISLANDS, JAPAN REGION\n24  2021-05-10  10:52:36   19.52   155.29          ISLAND OF HAWAII, HAWAII\n25  2021-05-10  10:47:23   44.28   115.19                    SOUTHERN IDAHO\n26  2021-05-10  10:37:51   42.80    13.22                     CENTRAL ITALY\n27  2021-05-10  10:35:02   44.49    19.90                            SERBIA\n28  2021-05-10  10:28:44   19.23   155.42          ISLAND OF HAWAII, HAWAII\n29  2021-05-10  10:18:30    9.06    83.85                        COSTA RICA\n30  2021-05-10  10:13:44   14.23    91.83                         GUATEMALA\n31  2021-05-10  10:08:52   44.35   115.19                    SOUTHERN IDAHO\n32  2021-05-10  10:07:57   31.70   104.15                     WESTERN TEXAS\n33  2021-05-10  10:06:05   19.59    69.53         DOMINICAN REPUBLIC REGION\n34  2021-05-10  10:00:54   42.27    76.33                        KYRGYZSTAN\n35  2021-05-10  09:53:19   16.55    98.47                  GUERRERO, MEXICO\n36  2021-05-10  09:48:59   43.35    12.52                     CENTRAL ITALY\n37  2021-05-10  09:47:05   25.56   129.91     NORTHERN TERRITORY, AUSTRALIA\n38  2021-05-10  09:41:54   36.41    27.25  DODECANESE IS.-TURKEY BORDER REG\n39  2021-05-10  09:32:12   28.52    68.07               LA RIOJA, ARGENTINA\n40  2021-05-10  09:25:02   67.61   159.00                   NORTHERN ALASKA\n41  2021-05-10  09:24:55   19.23   155.41          ISLAND OF HAWAII, HAWAII\n42  2021-05-10  09:22:20    4.79    80.98        PERU-ECUADOR BORDER REGION\n43  2021-05-10  09:19:30   19.20   155.41          ISLAND OF HAWAII, HAWAII\n44  2021-05-10  09:14:36   39.68    25.73                        AEGEAN SEA\n45  2021-05-10  09:04:53   35.62   117.58               SOUTHERN CALIFORNIA\n46  2021-05-10  09:04:20   17.19    99.85                  GUERRERO, MEXICO\n47  2021-05-10  08:50:30    4.24   131.58                         BANDA SEA\n48  2021-05-10  08:50:05   13.53   119.65         PHILIPPINE ISLANDS REGION\n49  2021-05-10  08:39:36   19.20   155.46          ISLAND OF HAWAII, HAWAII\n"
     ]
    }
   ],
   "source": [
    "colnames = ['date','time','latitud','longitud','region name']\n",
    "df.columns = colnames\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The Free Encyclopedia', 'English', '日本語', 'Español', 'Deutsch', 'Русский', 'Français', 'Italiano', '中文', 'Português', 'Polski', '\\n\\nDownload Wikipedia for Android or iOS\\n\\n']\n['6295000+ articles', '1267000+ 記事', '1681000+ artículos', '2574000+ Artikel', '1722000+ статей', '2326000+ articles', '1691000+ voci', '1195000+ 條目', '1066000+ artigos', '1472000+ haseł', 'This page is available under the Creative Commons Attribution-ShareAlike License', 'Terms of Use', 'Privacy Policy']\n"
     ]
    }
   ],
   "source": [
    "tags = ['strong']\n",
    "languages = [element.text.replace('\\xa0','') for element in soup.find_all(tags)]\n",
    "tags = ['small']\n",
    "articles = [element.text.replace('\\xa0','') for element in soup.find_all(tags)]\n",
    "print(languages)\n",
    "print(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Español', '1681000'),\n",
       " ('English', '6295000'),\n",
       " ('日本語', '1267000'),\n",
       " ('Deutsch', '2574000'),\n",
       " ('Русский', '1722000'),\n",
       " ('Français', '2326000'),\n",
       " ('Italiano', '1691000'),\n",
       " ('中文', '1195000'),\n",
       " ('Português', '1066000'),\n",
       " ('Polski', '1472000')]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "articles2 = []\n",
    "for article in articles:\n",
    "    articles2.append(article[:7])\n",
    "articles3 = articles2[:10]\n",
    "article_tmp  = articles3[0]\n",
    "article_tmp2 = articles3[1]\n",
    "articles3[0] = articles3[2]\n",
    "articles3[1] = article_tmp\n",
    "articles3[2] = article_tmp2\n",
    "languages2 = languages[1:11]\n",
    "language_tmp  = languages2[0]\n",
    "language_tmp2 = languages2[1]\n",
    "languages2[0] = languages2[2]\n",
    "languages2[1] = language_tmp\n",
    "languages2[2] = language_tmp2\n",
    "wiki = zip(languages2,articles3)\n",
    "list(wiki)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "#your code \n",
    "class_ = soup.select('h3[class=\"govuk-heading-s dgu-topics__heading\"] ')\n",
    "class_clean = [rev.text for rev in class_]\n",
    "class_clean = [element.strip().replace(\"\\n\",\"\") for element in  class_clean]\n",
    "class_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "import lxml.html as lh\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html = requests.get(url)\n",
    "soup = lh.fromstring(html.content)\n",
    "rows = soup.xpath('//tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<Element tr at 0x171bc520540>,\n",
       " <Element tr at 0x171bbfbc040>,\n",
       " <Element tr at 0x171bbfbc090>,\n",
       " <Element tr at 0x171bbfbc270>,\n",
       " <Element tr at 0x171bbfbc310>,\n",
       " <Element tr at 0x171bbfbc2c0>,\n",
       " <Element tr at 0x171bbfbc3b0>,\n",
       " <Element tr at 0x171bbfbc400>,\n",
       " <Element tr at 0x171bbfbc450>,\n",
       " <Element tr at 0x171bbfbc360>,\n",
       " <Element tr at 0x171bbfbc4f0>,\n",
       " <Element tr at 0x171bbfbc4a0>,\n",
       " <Element tr at 0x171bbfbc590>,\n",
       " <Element tr at 0x171bbfbc5e0>,\n",
       " <Element tr at 0x171bbfbc630>,\n",
       " <Element tr at 0x171bbfbc8b0>,\n",
       " <Element tr at 0x171bbfbcb80>,\n",
       " <Element tr at 0x171bbfb3a40>,\n",
       " <Element tr at 0x171bbfb39f0>,\n",
       " <Element tr at 0x171bbfb3f40>,\n",
       " <Element tr at 0x171bbfb3ef0>,\n",
       " <Element tr at 0x171bbfb3e50>,\n",
       " <Element tr at 0x171bbfb3e00>,\n",
       " <Element tr at 0x171bbfb3cc0>,\n",
       " <Element tr at 0x171bbfb3d60>,\n",
       " <Element tr at 0x171bbfb3f90>,\n",
       " <Element tr at 0x171bbfb3ae0>,\n",
       " <Element tr at 0x171ba8963b0>,\n",
       " <Element tr at 0x171bc0dee50>,\n",
       " <Element tr at 0x171bc0def90>,\n",
       " <Element tr at 0x171bc0dee00>,\n",
       " <Element tr at 0x171bc22c6d0>,\n",
       " <Element tr at 0x171bc22c720>,\n",
       " <Element tr at 0x171bc22c1d0>,\n",
       " <Element tr at 0x171bc22c3b0>,\n",
       " <Element tr at 0x171bc22c360>,\n",
       " <Element tr at 0x171bc22c310>,\n",
       " <Element tr at 0x171bc22c2c0>,\n",
       " <Element tr at 0x171bc22c270>,\n",
       " <Element tr at 0x171bc22c220>,\n",
       " <Element tr at 0x171bc22c040>,\n",
       " <Element tr at 0x171bc22cae0>,\n",
       " <Element tr at 0x171bc22c680>,\n",
       " <Element tr at 0x171bc22c770>,\n",
       " <Element tr at 0x171bc22c7c0>,\n",
       " <Element tr at 0x171bc22c810>,\n",
       " <Element tr at 0x171bc22cc70>,\n",
       " <Element tr at 0x171bc22cb30>,\n",
       " <Element tr at 0x171bc22ccc0>,\n",
       " <Element tr at 0x171bc22cd10>,\n",
       " <Element tr at 0x171bc22cd60>,\n",
       " <Element tr at 0x171bc22cdb0>,\n",
       " <Element tr at 0x171bc22ce00>,\n",
       " <Element tr at 0x171bc22ce50>,\n",
       " <Element tr at 0x171bc22cea0>,\n",
       " <Element tr at 0x171bc22cef0>,\n",
       " <Element tr at 0x171bc22cf40>,\n",
       " <Element tr at 0x171bc22cf90>,\n",
       " <Element tr at 0x171bc40a040>,\n",
       " <Element tr at 0x171bc40a090>,\n",
       " <Element tr at 0x171bc40a0e0>,\n",
       " <Element tr at 0x171bc40a130>,\n",
       " <Element tr at 0x171bc40a180>,\n",
       " <Element tr at 0x171bc40a1d0>,\n",
       " <Element tr at 0x171bc40a220>,\n",
       " <Element tr at 0x171bc40a270>,\n",
       " <Element tr at 0x171bc40a2c0>,\n",
       " <Element tr at 0x171bc40a310>,\n",
       " <Element tr at 0x171bc40a360>,\n",
       " <Element tr at 0x171bc40a3b0>,\n",
       " <Element tr at 0x171bc40a400>,\n",
       " <Element tr at 0x171bc40a450>,\n",
       " <Element tr at 0x171bc40a4a0>,\n",
       " <Element tr at 0x171bc40a4f0>,\n",
       " <Element tr at 0x171bc40a540>,\n",
       " <Element tr at 0x171bc40a590>,\n",
       " <Element tr at 0x171bc40a5e0>,\n",
       " <Element tr at 0x171bc40a630>,\n",
       " <Element tr at 0x171bc40a680>,\n",
       " <Element tr at 0x171bc40a6d0>,\n",
       " <Element tr at 0x171bc40a720>,\n",
       " <Element tr at 0x171bc40a770>,\n",
       " <Element tr at 0x171bc40a7c0>,\n",
       " <Element tr at 0x171bc40a810>,\n",
       " <Element tr at 0x171bc40a860>,\n",
       " <Element tr at 0x171bc40a8b0>,\n",
       " <Element tr at 0x171bc40a900>,\n",
       " <Element tr at 0x171bc40a950>,\n",
       " <Element tr at 0x171bc40a9a0>,\n",
       " <Element tr at 0x171bc40a9f0>,\n",
       " <Element tr at 0x171bc40aa40>,\n",
       " <Element tr at 0x171bc40aa90>]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# [len(row) for row in rows]\n",
    "language = [row for row in rows if len(row)==6]\n",
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for row in language:\n",
    "    cols = []\n",
    "    for col in row:\n",
    "        dato = col.text_content()\n",
    "        cols.append(dato.strip())\n",
    "    rows.append(cols)\n",
    "rows = rows[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df = df[[0,1,2]]\n",
    "# df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Rank                            Language Speackers(millions)\n0    1                    Mandarin Chinese                 918\n1    2                             Spanish                 480\n2    3                             English                 379\n3    4  Hindi (sanskritised Hindustani)[9]                 341\n4    5                             Bengali                 228\n5    6                          Portuguese                 221\n6    7                             Russian                 154\n7    8                            Japanese                 128\n8    9                 Western Punjabi[10]                92.7\n9   10                             Marathi                83.1\n"
     ]
    }
   ],
   "source": [
    "colnames = ['Rank','Language','Speackers(millions)']\n",
    "df.columns = colnames\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python395jvsc74a57bd0376f5b0795f81aa7b5d99e448d6274743a66c9892fd26cf11e505993f41df5fa",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}