{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import html5lib\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html data-a11y-animated-images=\"system\" data-color-mode=\"auto\" data-dark-theme=\"dark\" data-light-theme=\"light\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <link href=\"https://github.githubassets.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://avatars.githubusercontent.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://github-cloud.s3.amazonaws.com\" rel=\"dns-prefetch\"/>\n",
      "  <link href=\"https://user-images.githubusercontent.com/\" rel=\"dns-prefetch\"/>\n",
      "  <link crossorigin=\"\" href=\"https://github.githubassets.com\" rel=\"preconnect\"/>\n",
      "  <link href=\"https://avatars.githubusercontent.com\" rel=\"preconnect\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/light-719f1193e0c0.css\" media=\"all\" rel=\"stylesheet\"/>\n",
      "  <link crossorigin=\"anonymous\" href=\"https://github.githubassets.com/assets/dark-0c343b529849.css\" media=\"all\" rel=\"stylesheet\"/>\n",
      "  <link crossorigin=\"anonymous\" data-color-theme=\"dark_dimmed\" data-href=\"https://github.githubassets.com/assets/dar\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content)\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ArthurVickers', 'ajcvickers'), ('chencheng(云谦)', 'sorrycc'), ('NikitaSobolev', 'sobolevn'), ('LiWei', 'monkeyWie'), ('EllianaMay', 'Mause'), ('BasNijholt', 'basnijholt'), ('EmilErnerfeldt', 'emilk'), ('PedroCuenca', 'pcuenca'), ('StephenCelis', 'stephencelis'), ('WangchongZhou', 'fffonion'), ('BenjaminPasero', 'bpasero'), ('吴晟WuSheng', 'wu-sheng'), ('JonnyBorges', 'jonataslaw'), ('RichardHughes', 'hughsie'), ('AmmarAhmed', 'ammarahm-ed'), ('AlexRogozhnikov', 'arogozhnikov'), ('TobiasKlauser', 'tklauser'), ('StefanProdan', 'stefanprodan'), ('MartinAeschlimann', 'aeschli'), ('SteveMacenski', 'SteveMacenski'), ('NatalieWeizenbaum', 'nex3'), ('ZeeshanAhmad', 'ziishaned'), ('AdamJohnson', 'adamchainz'), ('ChristianDietrich', 'cdietrich'), ('JanoshRiebesell', 'janosh')]\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "listanombres = [tag.text for tag in soup.select('h1.h3')]\n",
    "listanick = [tag.text for tag in soup.select('p.f4')]\n",
    "del listanick[0]\n",
    "\n",
    "listanombres = [i.replace('\\n','').replace(' ','') for i in listanombres]\n",
    "listanick = [i.replace('\\n','').replace(' ','') for i in listanick]\n",
    "\n",
    "listadef = zip(listanombres , listanick)\n",
    "print(list(listadef))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hpcaitech/ColossalAI',\n",
       " 'vinta/awesome-python',\n",
       " 'PaddlePaddle/PaddleDetection',\n",
       " 'bregman-arie/devops-exercises',\n",
       " '521xueweihan/HelloGitHub',\n",
       " 'pre-commit/pre-commit-hooks',\n",
       " 'public-apis/public-apis',\n",
       " 'microsoft/recommenders',\n",
       " 'd8ahazard/sd_dreambooth_extension',\n",
       " 'open-mmlab/mmdeploy',\n",
       " 'mli/autocut',\n",
       " 'huggingface/setfit',\n",
       " 'SpyGuard/SpyGuard',\n",
       " 'GerryDazoo/Slinger',\n",
       " 'sczhou/CodeFormer',\n",
       " 'mouredev/Hello-Python',\n",
       " 'ihabunek/toot',\n",
       " 'ansible/awx',\n",
       " 'jackfrued/Python-100-Days',\n",
       " 'python/cpython',\n",
       " 'PaddlePaddle/PaddleSpeech',\n",
       " 'PaddlePaddle/PaddleOCR',\n",
       " 'microsoft/nni',\n",
       " 'Azure/azure-sdk-for-python',\n",
       " 'streamlit/streamlit']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code a.data\n",
    "linkrepo = requests.get(url)\n",
    "#linkrepo.status_code\n",
    "sopa = BeautifulSoup(linkrepo.content)\n",
    "repos = [tag.text for tag in sopa.select('h1.h3')]\n",
    "repos = [i.replace('\\n','').replace(' ','') for i in repos]\n",
    "repos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/30px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/38px-Wikisource-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/34px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/15px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/14px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/16px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/18px-Wikisource-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/21px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code img\n",
    "l = requests.get(url)\n",
    "sopax = BeautifulSoup(l.content)\n",
    "\n",
    "a = sopax.select('img')\n",
    "\n",
    "c = [tag['src'] for tag in a]\n",
    "\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " '#mw-head',\n",
       " '#searchInput',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '#Snakes',\n",
       " '#Computing',\n",
       " '#People',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/w/index.php?title=Python&action=edit&section=1',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/wiki/Colt_Python',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " '/wiki/Python_(codename)',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/wiki/Timon_of_Phlius',\n",
       " '/w/index.php?title=Python&action=edit&section=8',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/Pithon',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1120123765',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Human_name_disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " '/wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Wikipedia:Contents',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " '/wiki/Wikipedia:About',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Help:Introduction',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/w/index.php?title=Python&oldid=1120123765',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=1120123765&wpFormIdentifier=titleform',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " '/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//creativecommons.org/licenses/by-sa/3.0/',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://developer.wikimedia.org',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "l = requests.get(url)\n",
    "sopax = BeautifulSoup(l.content)\n",
    "a = sopax.select('a')\n",
    "\n",
    "c = [tag.get('href') for tag in a]\n",
    "c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "link = requests.get(url)\n",
    "#link.status_code\n",
    "usa = BeautifulSoup(link.content , 'html.parser')\n",
    "a = usa.select('div.usctitlechanged')\n",
    "\n",
    "len(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARNOLDO JIMENEZ',\n",
       " 'OMAR ALEXANDER CARDENAS',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'MICHAEL JAMES PRATT',\n",
       " 'RUJA IGNATOVA',\n",
       " 'RAFAEL CARO-QUINTERO']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code h3.title\n",
    "link = requests.get(url)\n",
    "#link.status_code\n",
    "fbi = BeautifulSoup(link.content , 'html.parser')\n",
    "a = fbi.select('h3.title')\n",
    "\n",
    "[tag.text.replace('\\n','') for tag in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url1 = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[    0                                                  1  \\\n",
       " 0 NaN  set_server_date(2022,11,13,21,11,10) Current t...   \n",
       " \n",
       "                         2  \n",
       " 0  Member access Name Pwd  ,\n",
       "                0              1\n",
       " 0  Member access  Member access\n",
       " 1            NaN            NaN\n",
       " 2           Name            NaN\n",
       " 3            NaN            NaN\n",
       " 4            Pwd            NaN\n",
       " 5            NaN            NaN\n",
       " 6            NaN            NaN,\n",
       "                0              1\n",
       " 0  Member access  Member access\n",
       " 1            NaN            NaN\n",
       " 2           Name            NaN\n",
       " 3            NaN            NaN\n",
       " 4            Pwd            NaN\n",
       " 5            NaN            NaN\n",
       " 6            NaN            NaN,\n",
       "                                                    0         1\n",
       " 0  - Sorting by column is performed on the data o...  Glossary,\n",
       "    CitizenResponse                                  \\\n",
       "      12345678910›» 12345678910›».1 12345678910›».2   \n",
       " 0                1             NaN               F   \n",
       " 1              NaN             NaN             NaN   \n",
       " 2              NaN             NaN             NaN   \n",
       " 3              NaN             NaN             NaN   \n",
       " 4              NaN             NaN             NaN   \n",
       " 5              NaN             NaN             NaN   \n",
       " 6              NaN             NaN             NaN   \n",
       " 7              NaN             NaN             NaN   \n",
       " 8              NaN             NaN             NaN   \n",
       " 9              NaN             NaN             NaN   \n",
       " 10               5             NaN              IV   \n",
       " 11             NaN             NaN             NaN   \n",
       " 12             NaN             NaN             NaN   \n",
       " 13             NaN             NaN             NaN   \n",
       " 14             NaN             NaN             NaN   \n",
       " 15             NaN             NaN             NaN   \n",
       " 16             NaN             NaN             NaN   \n",
       " 17             NaN             NaN             NaN   \n",
       " 18             NaN             NaN             NaN   \n",
       " 19             NaN             NaN             NaN   \n",
       " 20             NaN             NaN             NaN   \n",
       " 21             NaN             NaN             NaN   \n",
       " 22             NaN             NaN             NaN   \n",
       " 23             NaN             NaN             NaN   \n",
       " 24               2             NaN               F   \n",
       " 25             NaN             NaN             NaN   \n",
       " 26             NaN             NaN             NaN   \n",
       " 27             NaN             NaN             NaN   \n",
       " 28             NaN             NaN             NaN   \n",
       " 29             NaN             NaN             NaN   \n",
       " 30               1             NaN               F   \n",
       " 31             NaN             NaN             NaN   \n",
       " 32             NaN             NaN             NaN   \n",
       " 33             NaN             NaN             NaN   \n",
       " 34             NaN             NaN             NaN   \n",
       " 35             NaN             NaN             NaN   \n",
       " 36             NaN             NaN             NaN   \n",
       " 37             NaN             NaN             NaN   \n",
       " 38             NaN             NaN             NaN   \n",
       " 39             NaN             NaN             NaN   \n",
       " 40              10             NaN             III   \n",
       " 41             NaN             NaN             NaN   \n",
       " 42             NaN             NaN             NaN   \n",
       " 43             NaN             NaN             NaN   \n",
       " 44             NaN             NaN             NaN   \n",
       " 45             NaN             NaN             NaN   \n",
       " 46             NaN             NaN             NaN   \n",
       " 47             NaN             NaN             NaN   \n",
       " 48             NaN             NaN             NaN   \n",
       " 49               6             NaN               V   \n",
       " 50             NaN             NaN             NaN   \n",
       " 51   12345678910›»   12345678910›»   12345678910›»   \n",
       " 52             NaN             NaN             NaN   \n",
       " \n",
       "                        Date & Time UTC Latitude degrees                  \\\n",
       "                          12345678910›»    12345678910›» 12345678910›».1   \n",
       " 0       2022-11-13 20:55:02.016min ago            32.14               S   \n",
       " 1       2022-11-13 20:47:33.423min ago            43.25               N   \n",
       " 2       2022-11-13 20:44:42.026min ago            20.03               S   \n",
       " 3       2022-11-13 20:43:28.527min ago            18.61               N   \n",
       " 4       2022-11-13 20:41:20.029min ago            17.09               N   \n",
       " 5       2022-11-13 20:33:38.037min ago            13.62               N   \n",
       " 6       2022-11-13 20:27:40.443min ago            17.96               N   \n",
       " 7       2022-11-13 20:21:51.449min ago            35.51               N   \n",
       " 8       2022-11-13 20:20:36.050min ago            18.43               N   \n",
       " 9   2022-11-13 20:05:08.01hr 06min ago            24.01               S   \n",
       " 10  2022-11-13 19:49:19.01hr 21min ago            38.72               N   \n",
       " 11  2022-11-13 19:47:34.01hr 23min ago            11.95               N   \n",
       " 12  2022-11-13 19:44:31.21hr 26min ago            35.53               N   \n",
       " 13  2022-11-13 19:41:30.01hr 29min ago             6.72               S   \n",
       " 14  2022-11-13 19:40:08.01hr 31min ago            16.03               S   \n",
       " 15  2022-11-13 19:36:33.01hr 34min ago             0.78               N   \n",
       " 16  2022-11-13 19:25:08.01hr 46min ago             0.99               S   \n",
       " 17  2022-11-13 19:16:32.91hr 54min ago            36.99               N   \n",
       " 18  2022-11-13 19:06:32.02hr 04min ago            11.60               N   \n",
       " 19  2022-11-13 19:03:24.02hr 07min ago            19.42               N   \n",
       " 20  2022-11-13 18:53:24.02hr 17min ago            11.29               N   \n",
       " 21  2022-11-13 18:53:17.52hr 17min ago            45.05               N   \n",
       " 22  2022-11-13 18:52:35.02hr 18min ago            23.73               S   \n",
       " 23  2022-11-13 18:51:59.82hr 19min ago            28.59               N   \n",
       " 24  2022-11-13 18:51:18.12hr 19min ago            35.01               N   \n",
       " 25  2022-11-13 18:43:16.02hr 27min ago            30.42               S   \n",
       " 26  2022-11-13 18:40:37.02hr 30min ago             2.09               N   \n",
       " 27  2022-11-13 18:40:36.42hr 30min ago            35.52               N   \n",
       " 28  2022-11-13 18:36:30.02hr 34min ago            10.78               N   \n",
       " 29  2022-11-13 18:22:06.02hr 49min ago             9.69               S   \n",
       " 30  2022-11-13 18:13:58.02hr 57min ago             6.73               S   \n",
       " 31  2022-11-13 17:53:27.03hr 17min ago             2.00               N   \n",
       " 32  2022-11-13 17:51:26.03hr 19min ago             8.33               S   \n",
       " 33  2022-11-13 17:42:52.03hr 28min ago             2.04               S   \n",
       " 34  2022-11-13 17:39:27.63hr 31min ago            19.47               N   \n",
       " 35  2022-11-13 17:22:04.03hr 49min ago             3.97               S   \n",
       " 36  2022-11-13 16:55:57.04hr 15min ago            20.45               S   \n",
       " 37  2022-11-13 16:54:33.04hr 16min ago             8.65               S   \n",
       " 38  2022-11-13 16:37:39.34hr 33min ago             8.25               S   \n",
       " 39  2022-11-13 16:33:26.04hr 37min ago            12.22               N   \n",
       " 40  2022-11-13 16:33:16.14hr 37min ago            45.87               N   \n",
       " 41  2022-11-13 16:22:43.84hr 48min ago            12.94               N   \n",
       " 42  2022-11-13 16:09:59.05hr 01min ago            17.01               N   \n",
       " 43  2022-11-13 16:05:46.25hr 05min ago            37.16               N   \n",
       " 44  2022-11-13 16:04:05.05hr 07min ago            34.42               S   \n",
       " 45  2022-11-13 16:01:38.55hr 09min ago            19.14               N   \n",
       " 46  2022-11-13 15:52:44.05hr 18min ago            37.55               S   \n",
       " 47  2022-11-13 15:44:45.95hr 26min ago            33.63               N   \n",
       " 48  2022-11-13 15:44:33.05hr 26min ago            18.59               S   \n",
       " 49  2022-11-13 15:41:15.05hr 29min ago             6.77               S   \n",
       " 50                                 NaN              NaN             NaN   \n",
       " 51                       12345678910›»    12345678910›»   12345678910›»   \n",
       " 52                                 NaN              NaN             NaN   \n",
       " \n",
       "    Longitude degrees                       Depth km        Mag [+]  \\\n",
       "        12345678910›» 12345678910›».1  12345678910›»  12345678910›»   \n",
       " 0              72.05               W             30              M   \n",
       " 1             127.00               W             10             ML   \n",
       " 2              70.42               W             34              M   \n",
       " 3              66.40               W             70             ML   \n",
       " 4             120.14               E            219              M   \n",
       " 5             125.31               E             18              M   \n",
       " 6              66.96               W             11             Md   \n",
       " 7               3.62               W              9             ML   \n",
       " 8             121.14               E             77              M   \n",
       " 9              67.95               W            249              M   \n",
       " 10             20.62               E              1             ML   \n",
       " 11             87.17               W             29              M   \n",
       " 12              3.62               W              2             ML   \n",
       " 13            107.37               E             10              M   \n",
       " 14             73.82               W             40              M   \n",
       " 15            121.66               E             10              M   \n",
       " 16            100.15               E             44              M   \n",
       " 17            122.24               W              8             Md   \n",
       " 18             87.44               W             32              M   \n",
       " 19            155.28               W             -1             Ml   \n",
       " 20             85.66               W            169              M   \n",
       " 21             25.79               E             27             ML   \n",
       " 22             68.79               W            107              M   \n",
       " 23             17.90               W              8             ML   \n",
       " 24             26.37               E              2             ML   \n",
       " 25             71.27               W             51              M   \n",
       " 26            129.28               E             10              M   \n",
       " 27              3.58               W             16             ML   \n",
       " 28             85.70               W             24              M   \n",
       " 29            119.68               E             45              M   \n",
       " 30            107.35               E             10              M   \n",
       " 31            127.20               E             91              M   \n",
       " 32            116.97               E             10              M   \n",
       " 33            121.23               E             10              M   \n",
       " 34            155.61               W             -2             ML   \n",
       " 35            141.72               E             79              M   \n",
       " 36             69.07               W            101              M   \n",
       " 37            116.98               E            120              M   \n",
       " 38            158.79               E            100             mb   \n",
       " 39             87.97               W             24              M   \n",
       " 40              6.84               E              6             ML   \n",
       " 41             49.44               E             80             mb   \n",
       " 42            120.71               E            121              M   \n",
       " 43              4.06               W             11             ML   \n",
       " 44             70.63               W            107              M   \n",
       " 45            155.49               W             35             Ml   \n",
       " 46             73.81               W             22              M   \n",
       " 47             82.03               E             10             mb   \n",
       " 48             71.07               W             34              M   \n",
       " 49            107.34               E             10              M   \n",
       " 50               NaN             NaN            NaN            NaN   \n",
       " 51     12345678910›»   12345678910›»  12345678910›»  12345678910›»   \n",
       " 52               NaN             NaN            NaN            NaN   \n",
       " \n",
       "    Region name [+]                Last update [-] Unnamed: 12_level_0  \n",
       "      12345678910›»                  12345678910›»       12345678910›»  \n",
       " 0              4.3       OFFSHORE COQUIMBO, CHILE    2022-11-13 20:57  \n",
       " 1              3.0            OFF COAST OF OREGON    2022-11-13 20:59  \n",
       " 2              2.6       OFFSHORE TARAPACA, CHILE    2022-11-13 20:52  \n",
       " 3              2.8             PUERTO RICO REGION    2022-11-13 21:04  \n",
       " 4              3.1             LUZON, PHILIPPINES    2022-11-13 20:55  \n",
       " 5              4.2      PHILIPPINE ISLANDS REGION    2022-11-13 20:50  \n",
       " 6              2.1                    PUERTO RICO    2022-11-13 20:41  \n",
       " 7              1.8            STRAIT OF GIBRALTAR    2022-11-13 20:33  \n",
       " 8              3.0             LUZON, PHILIPPINES    2022-11-13 20:35  \n",
       " 9              3.5             ANTOFAGASTA, CHILE    2022-11-13 20:18  \n",
       " 10             2.8                         GREECE    2022-11-13 20:10  \n",
       " 11             2.8        NEAR COAST OF NICARAGUA    2022-11-13 19:55  \n",
       " 12             2.0            STRAIT OF GIBRALTAR    2022-11-13 19:52  \n",
       " 13             2.9                JAVA, INDONESIA    2022-11-13 19:50  \n",
       " 14             4.0    NEAR COAST OF SOUTHERN PERU    2022-11-13 19:50  \n",
       " 15             3.0  MINAHASA, SULAWESI, INDONESIA    2022-11-13 19:50  \n",
       " 16             2.8    SOUTHERN SUMATRA, INDONESIA    2022-11-13 19:50  \n",
       " 17             2.1   OFFSHORE NORTHERN CALIFORNIA    2022-11-13 19:18  \n",
       " 18             4.1        NEAR COAST OF NICARAGUA    2022-11-13 19:15  \n",
       " 19             2.1       ISLAND OF HAWAII, HAWAII    2022-11-13 19:08  \n",
       " 20             3.1                      NICARAGUA    2022-11-13 19:00  \n",
       " 21             2.5                        ROMANIA    2022-11-13 19:33  \n",
       " 22             2.7             ANTOFAGASTA, CHILE    2022-11-13 19:16  \n",
       " 23             1.9   CANARY ISLANDS, SPAIN REGION    2022-11-13 18:55  \n",
       " 24             3.6                  CRETE, GREECE    2022-11-13 19:19  \n",
       " 25             2.7                COQUIMBO, CHILE    2022-11-13 18:51  \n",
       " 26             3.9           HALMAHERA, INDONESIA    2022-11-13 19:00  \n",
       " 27             1.8            STRAIT OF GIBRALTAR    2022-11-13 18:47  \n",
       " 28             4.0                     COSTA RICA    2022-11-13 19:00  \n",
       " 29             2.9        SUMBA REGION, INDONESIA    2022-11-13 18:35  \n",
       " 30             3.3                JAVA, INDONESIA    2022-11-13 18:20  \n",
       " 31             3.2                    MOLUCCA SEA    2022-11-13 18:00  \n",
       " 32             2.5       LOMBOK REGION, INDONESIA    2022-11-13 18:05  \n",
       " 33             2.8            SULAWESI, INDONESIA    2022-11-13 18:10  \n",
       " 34             2.1       ISLAND OF HAWAII, HAWAII    2022-11-13 17:45  \n",
       " 35             3.8   NEW GUINEA, PAPUA NEW GUINEA    2022-11-13 17:30  \n",
       " 36             3.4                TARAPACA, CHILE    2022-11-13 17:26  \n",
       " 37             2.7       LOMBOK REGION, INDONESIA    2022-11-13 17:05  \n",
       " 38             4.7                SOLOMON ISLANDS    2022-11-13 17:26  \n",
       " 39             2.8        NEAR COAST OF NICARAGUA    2022-11-13 16:41  \n",
       " 40             2.5                         FRANCE    2022-11-13 17:09  \n",
       " 41             4.5                   GULF OF ADEN    2022-11-13 18:24  \n",
       " 42             3.1             LUZON, PHILIPPINES    2022-11-13 16:20  \n",
       " 43             1.8                          SPAIN    2022-11-13 16:12  \n",
       " 44             2.7    LIBERTADOR O'HIGGINS, CHILE    2022-11-13 16:26  \n",
       " 45             2.0       ISLAND OF HAWAII, HAWAII    2022-11-13 16:07  \n",
       " 46             2.7        OFFSHORE BIO-BIO, CHILE    2022-11-13 16:15  \n",
       " 47             4.1                 WESTERN XIZANG    2022-11-13 16:19  \n",
       " 48             2.9   OFF COAST OF TARAPACA, CHILE    2022-11-13 16:06  \n",
       " 49             4.1                JAVA, INDONESIA    2022-11-13 15:50  \n",
       " 50             NaN                            NaN                 NaN  \n",
       " 51   12345678910›»                  12345678910›»       12345678910›»  \n",
       " 52             NaN                            NaN                 NaN  ]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "link1 = requests.get(url1)\n",
    "#link.status_code\n",
    "quake = BeautifulSoup(link1.content , 'html.parser')\n",
    "x = pd.read_html(str(quake.select('table')))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 ='https://mlh.io/seasons/na-2020/events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Event Date</th>\n",
       "      <th>Event Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HackMTY</td>\n",
       "      <td>Aug24th-25th</td>\n",
       "      <td>Monterrey,MX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Citizen Hacks</td>\n",
       "      <td>Sep6th-8th</td>\n",
       "      <td>Toronto,ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PennApps</td>\n",
       "      <td>Sep6th-8th</td>\n",
       "      <td>Philadelphia,PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hackathon de Futuras Tecnologías</td>\n",
       "      <td>Sep7th-8th</td>\n",
       "      <td>Torreón,MX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hack the North</td>\n",
       "      <td>Sep13th-15th</td>\n",
       "      <td>Waterloo,ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Same Home Different Hacks</td>\n",
       "      <td>Jun12th-14th</td>\n",
       "      <td>Everywhere,Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Hack Girl Summer</td>\n",
       "      <td>Jun19th-21st</td>\n",
       "      <td>Everywhere,Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Def Hacks Virtual</td>\n",
       "      <td>Jun20th-21st</td>\n",
       "      <td>Everywhere,Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Data Day Grind</td>\n",
       "      <td>Jun27th-28th</td>\n",
       "      <td>Everywhere,Worldwide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>HackTJ</td>\n",
       "      <td>Dec12th-13th</td>\n",
       "      <td>Alexandria,VA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Event Name    Event Date        Event Location\n",
       "0                               HackMTY  Aug24th-25th          Monterrey,MX\n",
       "1                         Citizen Hacks    Sep6th-8th            Toronto,ON\n",
       "2                              PennApps    Sep6th-8th       Philadelphia,PA\n",
       "3     Hackathon de Futuras Tecnologías     Sep7th-8th            Torreón,MX\n",
       "4                        Hack the North  Sep13th-15th           Waterloo,ON\n",
       "..                                  ...           ...                   ...\n",
       "146           Same Home Different Hacks  Jun12th-14th  Everywhere,Worldwide\n",
       "147                    Hack Girl Summer  Jun19th-21st  Everywhere,Worldwide\n",
       "148                   Def Hacks Virtual  Jun20th-21st  Everywhere,Worldwide\n",
       "149                      Data Day Grind  Jun27th-28th  Everywhere,Worldwide\n",
       "150                              HackTJ  Dec12th-13th         Alexandria,VA\n",
       "\n",
       "[151 rows x 3 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link1 = requests.get(url2)\n",
    "sopa = BeautifulSoup(link1.content)\n",
    "hack = sopa.select('h3.event-name')\n",
    "\n",
    "event_name = [tag.text for tag in hack]\n",
    "#event = [i.replace('\\n','') for i in event]\n",
    "#event.split(' ')\n",
    "event_name\n",
    "\n",
    "hack2 = sopa.select('p.event-date')\n",
    "\n",
    "event_date = [tag.text for tag in hack2]\n",
    "event_date = [i.replace(' ','') for i in event_date]\n",
    "event_date\n",
    "\n",
    "hack3 = sopa.select('div.event-location')\n",
    "\n",
    "event_location = [tag.text for tag in hack3]\n",
    "event_location = [i.replace(' ','').replace('\\n','') for i in event_location]\n",
    "event_location\n",
    "\n",
    "dicti = {'Event Name' : event_name , 'Event Date' : event_date , 'Event Location' : event_location}\n",
    "\n",
    "df = pd.DataFrame(dicti)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url3 = 'https://twitter.com/elonmusk?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usuario invalido\n"
     ]
    }
   ],
   "source": [
    "# esto funciona, pero cuando se vuelve a cargar el cuaderno falla\n",
    "\n",
    "usuario = input()\n",
    "url = f'https://twitter.com/{user}'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    source = driver.page_source\n",
    "    sopa = BeautifulSoup(source)\n",
    "    tweets = sopa.select(\"div [class = 'css-1dbjc4n r-1habvwh'] > div[dir='auto']\")\n",
    "    num = tweets[0].text\n",
    "    driver.quit()\n",
    "    print(num)\n",
    "    \n",
    "except:\n",
    "    print('usuario invalido')\n",
    "    driver.quit()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " amazon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No te pierdas lo que está pasandoLos usuarios de Twitter son los primeros en enterarse.Iniciar sesiónRegístrate\n"
     ]
    }
   ],
   "source": [
    "#your code me obliga a registrarme\n",
    "user = input()\n",
    "url = f'https://twitter.com/{user}'\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    source = driver.page_source\n",
    "    soup = BeautifulSoup(source)\n",
    "    follow = soup.select('div.css-1dbjc4n')\n",
    "    followers = follow[7].text\n",
    "    driver.quit()\n",
    "    print(followers)\n",
    "    \n",
    "except:\n",
    "    print('usuario invalido')\n",
    "    driver.quit()\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 314 000 記事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 755 000 artículos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 400 000 articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 798 000 статей</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 667 000 Artikel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1 742 000 voci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1 256 000 条目 / 條目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>866 000 مقاله</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1 085 000 artigos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              articles\n",
       "0         1 314 000 記事\n",
       "1  1 755 000 artículos\n",
       "2   2 400 000 articles\n",
       "3     1 798 000 статей\n",
       "4    2 667 000 Artikel\n",
       "5       1 742 000 voci\n",
       "6    1 256 000 条目 / 條目\n",
       "7        866 000 مقاله\n",
       "8    1 085 000 artigos"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "link1 = requests.get(url)\n",
    "sopa = BeautifulSoup(link1.content)\n",
    "hack = sopa.select('small')\n",
    "\n",
    "event_name = [tag.text.replace('+','').replace(u'\\xa0', ' ') for tag in hack]\n",
    "\n",
    "event_name = event_name[1:10]\n",
    "\n",
    "df=pd.DataFrame(event_name ,columns=['articles'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "link1 = requests.get(url)\n",
    "sopa = BeautifulSoup(link1.content)\n",
    "hack = sopa.select('a.govuk-link')\n",
    "\n",
    "event_name = [tag.text.replace('+','').replace(u'\\xa0', ' ') for tag in hack]\n",
    "event_name[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Native speakers(millions)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese(incl. Standard Chinese, but e...</td>\n",
       "      <td>920.0</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>475.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>373.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi(excl. Urdu)</td>\n",
       "      <td>344.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>234.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>232.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>125.0</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Yue Chinese(incl. Cantonese)</td>\n",
       "      <td>85.2</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>84.6</td>\n",
       "      <td>Austroasiatic</td>\n",
       "      <td>Vietic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Telugu</td>\n",
       "      <td>82.7</td>\n",
       "      <td>Dravidian</td>\n",
       "      <td>South-Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Turkish</td>\n",
       "      <td>82.2</td>\n",
       "      <td>Turkic</td>\n",
       "      <td>Oghuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wu Chinese(incl. Shanghainese)</td>\n",
       "      <td>81.8</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Korean</td>\n",
       "      <td>81.7</td>\n",
       "      <td>Koreanic</td>\n",
       "      <td>language isolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>French</td>\n",
       "      <td>79.9</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tamil</td>\n",
       "      <td>78.4</td>\n",
       "      <td>Dravidian</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Standard German</td>\n",
       "      <td>75.6</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Egyptian Spoken Arabic(excl. Saʽidi Arabic)</td>\n",
       "      <td>74.8</td>\n",
       "      <td>Afroasiatic</td>\n",
       "      <td>Semitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Urdu(excl. Hindi)</td>\n",
       "      <td>70.2</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Javanese</td>\n",
       "      <td>68.3</td>\n",
       "      <td>Austronesian</td>\n",
       "      <td>Malayo-Polynesian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Western Punjabi(excl. Eastern Punjabi)</td>\n",
       "      <td>66.4</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Italian</td>\n",
       "      <td>64.8</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Gujarati</td>\n",
       "      <td>57.0</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Iranian Persian(excl. Dari and Tajik)</td>\n",
       "      <td>56.4</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Iranian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bhojpuri</td>\n",
       "      <td>52.3</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hausa</td>\n",
       "      <td>50.8</td>\n",
       "      <td>Afroasiatic</td>\n",
       "      <td>Chadic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Language  \\\n",
       "0   Mandarin Chinese(incl. Standard Chinese, but e...   \n",
       "1                                             Spanish   \n",
       "2                                             English   \n",
       "3                                   Hindi(excl. Urdu)   \n",
       "4                                             Bengali   \n",
       "5                                          Portuguese   \n",
       "6                                             Russian   \n",
       "7                                            Japanese   \n",
       "8                        Yue Chinese(incl. Cantonese)   \n",
       "9                                          Vietnamese   \n",
       "10                                            Marathi   \n",
       "11                                             Telugu   \n",
       "12                                            Turkish   \n",
       "13                     Wu Chinese(incl. Shanghainese)   \n",
       "14                                             Korean   \n",
       "15                                             French   \n",
       "16                                              Tamil   \n",
       "17                                    Standard German   \n",
       "18        Egyptian Spoken Arabic(excl. Saʽidi Arabic)   \n",
       "19                                  Urdu(excl. Hindi)   \n",
       "20                                           Javanese   \n",
       "21             Western Punjabi(excl. Eastern Punjabi)   \n",
       "22                                            Italian   \n",
       "23                                           Gujarati   \n",
       "24              Iranian Persian(excl. Dari and Tajik)   \n",
       "25                                           Bhojpuri   \n",
       "26                                              Hausa   \n",
       "\n",
       "    Native speakers(millions) Language family             Branch  \n",
       "0                       920.0    Sino-Tibetan            Sinitic  \n",
       "1                       475.0   Indo-European            Romance  \n",
       "2                       373.0   Indo-European           Germanic  \n",
       "3                       344.0   Indo-European         Indo-Aryan  \n",
       "4                       234.0   Indo-European         Indo-Aryan  \n",
       "5                       232.0   Indo-European            Romance  \n",
       "6                       154.0   Indo-European       Balto-Slavic  \n",
       "7                       125.0         Japonic           Japanese  \n",
       "8                        85.2    Sino-Tibetan            Sinitic  \n",
       "9                        84.6   Austroasiatic             Vietic  \n",
       "10                       83.1   Indo-European         Indo-Aryan  \n",
       "11                       82.7       Dravidian      South-Central  \n",
       "12                       82.2          Turkic              Oghuz  \n",
       "13                       81.8    Sino-Tibetan            Sinitic  \n",
       "14                       81.7        Koreanic   language isolate  \n",
       "15                       79.9   Indo-European            Romance  \n",
       "16                       78.4       Dravidian              South  \n",
       "17                       75.6   Indo-European           Germanic  \n",
       "18                       74.8     Afroasiatic            Semitic  \n",
       "19                       70.2   Indo-European         Indo-Aryan  \n",
       "20                       68.3    Austronesian  Malayo-Polynesian  \n",
       "21                       66.4   Indo-European         Indo-Aryan  \n",
       "22                       64.8   Indo-European            Romance  \n",
       "23                       57.0   Indo-European         Indo-Aryan  \n",
       "24                       56.4   Indo-European            Iranian  \n",
       "25                       52.3   Indo-European         Indo-Aryan  \n",
       "26                       50.8     Afroasiatic             Chadic  "
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "link1 = requests.get(url)\n",
    "sopa = BeautifulSoup(link1.content)\n",
    "#hack = sopa.select('table')\n",
    "x = pd.read_html(str(sopa.select('table')))\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rank &amp; Title</th>\n",
       "      <th>IMDb Rating</th>\n",
       "      <th>Your Rating</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.  The Shawshank Redemption (1994)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.  El padrino (1972)</td>\n",
       "      <td>9.2</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.  Batman 2: El caballero de la noche (2008)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.  El padrino II (1974)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.  Doce hombres en pugna (1957)</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>NaN</td>\n",
       "      <td>246.  Dersu Uzala (1975)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>NaN</td>\n",
       "      <td>247.  Aladdin (1992)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>NaN</td>\n",
       "      <td>248.  Historias cruzadas (2011)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>NaN</td>\n",
       "      <td>249.  Gandhi (1982)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>NaN</td>\n",
       "      <td>250.  The Iron Giant (1999)</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12345678910 NOT YET RELEASED  Seen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                   Rank & Title  IMDb Rating  \\\n",
       "0           NaN            1.  The Shawshank Redemption (1994)          9.2   \n",
       "1           NaN                          2.  El padrino (1972)          9.2   \n",
       "2           NaN  3.  Batman 2: El caballero de la noche (2008)          9.0   \n",
       "3           NaN                       4.  El padrino II (1974)          9.0   \n",
       "4           NaN               5.  Doce hombres en pugna (1957)          9.0   \n",
       "..          ...                                            ...          ...   \n",
       "245         NaN                       246.  Dersu Uzala (1975)          8.0   \n",
       "246         NaN                           247.  Aladdin (1992)          8.0   \n",
       "247         NaN                248.  Historias cruzadas (2011)          8.0   \n",
       "248         NaN                            249.  Gandhi (1982)          8.0   \n",
       "249         NaN                    250.  The Iron Giant (1999)          8.0   \n",
       "\n",
       "                            Your Rating  Unnamed: 4  \n",
       "0    12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "1    12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "2    12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "3    12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "4    12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "..                                  ...         ...  \n",
       "245  12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "246  12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "247  12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "248  12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "249  12345678910 NOT YET RELEASED  Seen         NaN  \n",
       "\n",
       "[250 rows x 5 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "link1 = requests.get(url)\n",
    "sopa = BeautifulSoup(link1.content)\n",
    "#hack = sopa.select('table')\n",
    "x = pd.read_html(str(sopa.select('table')))\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the city: a\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'\n",
    "\n",
    "#cod\":401, \"message\": \"Invalid API key. Please see https://openweathermap.org/faq#error401 for more info.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Precio</th>\n",
       "      <th>Disponibilidad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>Instock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Titulo  Precio Disponibilidad\n",
       "0                      A Light in the ...  £51.77        Instock\n",
       "1                      Tipping the Velvet  £53.74        Instock\n",
       "2                              Soumission  £50.10        Instock\n",
       "3                           Sharp Objects  £47.82        Instock\n",
       "4            Sapiens: A Brief History ...  £54.23        Instock\n",
       "5                         The Requiem Red  £22.65        Instock\n",
       "6            The Dirty Little Secrets ...  £33.34        Instock\n",
       "7                 The Coming Woman: A ...  £17.93        Instock\n",
       "8                     The Boys in the ...  £22.60        Instock\n",
       "9                         The Black Maria  £52.15        Instock\n",
       "10  Starving Hearts (Triangular Trade ...  £13.99        Instock\n",
       "11                  Shakespeare's Sonnets  £20.66        Instock\n",
       "12                            Set Me Free  £17.46        Instock\n",
       "13    Scott Pilgrim's Precious Little ...  £52.29        Instock\n",
       "14                      Rip it Up and ...  £35.02        Instock\n",
       "15                  Our Band Could Be ...  £57.25        Instock\n",
       "16                                   Olio  £23.88        Instock\n",
       "17        Mesaerion: The Best Science ...  £37.59        Instock\n",
       "18           Libertarianism for Beginners  £51.33        Instock\n",
       "19                It's Only the Himalayas  £45.17        Instock"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "link1 = requests.get(url)\n",
    "sopa = BeautifulSoup(link1.content)\n",
    "hack = sopa.select('h3')\n",
    "titulo = [tag.text for tag in hack]\n",
    "\n",
    "\n",
    "hack2 = sopa.select('p.price_color')\n",
    "titulo2 = [tag.text for tag in hack2]\n",
    "\n",
    "\n",
    "\n",
    "hack3 = sopa.select('.instock ')\n",
    "titulo3 = [tag.text.replace('\\n' , '').replace(' ' , '') for tag in hack3]\n",
    "\n",
    "dicti = {'Titulo' : titulo , 'Precio' : titulo2 , 'Disponibilidad' : titulo3}\n",
    "\n",
    "df = pd.DataFrame(dicti)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
